{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckbbj5diaHkg"
      },
      "source": [
        "# Fine-tuning Embeddings for RAG on Specific Data\n",
        "\n",
        "As we start our \"fine-tuning\" week, we'll start with the lowest hanging improvement one can do for RAG - which is:\n",
        "\n",
        "Fine-tuning embeddings!\n",
        "\n",
        "- 🤝 Breakout Room #1:\n",
        "  - Task 1: Dependencies and Boilerplate\n",
        "  - Task 2: Loading Data\n",
        "  - Task 3: Constructing a Fine-tuning Dataset\n",
        "  - Task 4: Fine-tuning `snowflake-arctic-embed-l`\n",
        "  - Task 5: Evaluating our Retriever\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xwor_3X6ODX"
      },
      "source": [
        "#### Basic Overview of Fine-tuning Embeddings\n",
        "\n",
        "In essence, what we want to do when we fine-tune our embedding models is very simple:\n",
        "\n",
        "```\n",
        "Move the embeddings for questions relating to a document\n",
        "closer together with that document\n",
        "```\n",
        "\n",
        "We can think of fine-tuning our embedding models as follows:\n",
        "\n",
        "1) We have some pair of text items that *should* be closer together\n",
        "  - `Question`, `Document` pairs\n",
        "  - EX: `Who drives the bus?`, `The bus was driven by Kyle, the Bus Driver`.\n",
        "\n",
        "2) We use these pairs as labeled data to fine-tune our embedding model.\n",
        "\n",
        "The process of training helps the model more accurately associate our questions with the correct documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX5R3HVz6FOQ"
      },
      "source": [
        "#####❓ Question #1:\n",
        "\n",
        "Describe the nuance between using Q&D pairs to train the embedding model vs. inter-document pairs/related sentences.\n",
        "\n",
        "What caveats does this approach have? Are there any special considerations for what kind of Q's we should use?\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER:**\n",
        "\n",
        "We are specifically relating *the questions* to *the documents*. This means that we are making our embedding model at the very specific task of relating potential questions to specific documents.\n",
        "\n",
        "There are many caveats, but the main ones are:\n",
        "\n",
        "- Your Q's should reflect the Q's of your users\n",
        "- This kind of fine-tuning will (purposefully) \"overfit\" on your data; this is the desired result in this case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NkSaurzbpyS"
      },
      "source": [
        "## Task 1: Dependencies and Boilerplate\n",
        "\n",
        "We'll set up our `nest_asyncio` so we can leverage async loops in our Notebook.\n",
        "\n",
        "We'll also install the required libraries we'll be using today, and set up our OpenAI API key!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c_EUibmcDU3"
      },
      "source": [
        "### Nest Asyncio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zq-6s7LbPnKH"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8uFz8RVcFFu"
      },
      "source": [
        "### Install Dependencies\n",
        "\n",
        ">> NOTE: You do not need to do these steps if you are running this notebook locally with `uv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulZIBA1ZoSsV",
        "outputId": "12d9c766-843f-40bf-bdf8-e0ed04b6d87f"
      },
      "outputs": [],
      "source": [
        "#!pip install -qU langchain_openai langchain_huggingface langchain_core langchain langchain_community langchain-text-splitters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3GFD7B-tOCrx"
      },
      "outputs": [],
      "source": [
        "#!pip install -qU faiss-cpu python-pptx==1.0.2 nltk==3.9.1 pymupdf beautifulsoup4 lxml "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FM-eUlrcI8a"
      },
      "source": [
        "### Provide OpenAI API Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wA_mlurVqtrp",
        "outputId": "18cccb1e-095f-40fa-def5-2454f9bcdcae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#import os\n",
        "#import getpass\n",
        "\n",
        "#os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter Your OpenAI API Key: \")\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFZ217gCDVTr"
      },
      "source": [
        "## Task 2: Loading Data\n",
        "\n",
        "We'll prepare our data - and download our webpages which we'll be using for our data today.\n",
        "\n",
        "These webpages are from [Simon Willison's](https://simonwillison.net/) yearly \"AI learnings\".\n",
        "\n",
        "- [2023 Blog](https://simonwillison.net/2023/Dec/31/ai-in-2023/)\n",
        "- [2024 Blog](https://simonwillison.net/2024/Dec/31/llms-in-2024/)\n",
        "\n",
        "Let's start by collecting our data into a useful pile!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: data: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 31413    0 31413    0     0   116k      0 --:--:-- --:--:-- --:--:--  116k\n"
          ]
        }
      ],
      "source": [
        "!curl https://simonwillison.net/2023/Dec/31/ai-in-2023/ -o data/2023_llms.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 70272    0 70272    0     0   129k      0 --:--:-- --:--:-- --:--:--  129k\n"
          ]
        }
      ],
      "source": [
        "!curl https://simonwillison.net/2024/Dec/31/llms-in-2024/ -o data/2024_llms.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DHJhTzsvN75t"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "from langchain_community.document_loaders import BSHTMLLoader\n",
        "\n",
        "path = \"data/\"\n",
        "text_loader = DirectoryLoader(path, glob=\"*.html\", loader_cls=BSHTMLLoader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UbKa6-V0nvp"
      },
      "source": [
        "Next, we'll set up a classic naive chunking strategy as we only care that the documents get parsed into chunks that we can generate synthetic questions about."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NsPrOOqXOsNX"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 750,\n",
        "    chunk_overlap  = 20,\n",
        "    length_function = len\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lf_PoX7l09Rg"
      },
      "source": [
        "Next we can load/split these documents as follows.\n",
        "\n",
        ">> NOTE: You may need to run this cell twice to get it to work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OMYPX6N6Os8M"
      },
      "outputs": [],
      "source": [
        "training_documents = text_splitter.split_documents(text_loader.load())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAozuMoNOvnp",
        "outputId": "dc1d663e-7153-4c51-cedb-d1bc3888c4ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "102"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(training_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yE2TFIq1BuJ"
      },
      "source": [
        "Next, we're going to associate each of our chunks with a unique identifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "AwyIForybIpo"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "\n",
        "id_set = set()\n",
        "\n",
        "for document in training_documents:\n",
        "  id = str(uuid.uuid4())\n",
        "  while id in id_set:\n",
        "    id = uuid.uuid4()\n",
        "  id_set.add(id)\n",
        "  document.metadata[\"id\"] = id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJnL4oNg341U"
      },
      "source": [
        "Next, we'll simply use naive Python slicing to create a training, test, and validation set to prepare our data for the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "MTS4GTSEcnG4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "78\n",
            "12\n",
            "12\n"
          ]
        }
      ],
      "source": [
        "training_split_documents = training_documents[:len(training_documents) - 24]\n",
        "val_split_documents = training_documents[len(training_documents) - 24:102-12]\n",
        "test_split_documents = training_documents[102-12:]\n",
        "print(len(training_split_documents))\n",
        "print(len(val_split_documents))\n",
        "print(len(test_split_documents))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzlvKbONDWvQ"
      },
      "source": [
        "## Task 3: Constructing a Fine-tuning Dataset\n",
        "\n",
        "Using the nodes we created above, we can finally start constructing a fine-tuning dataset utilizing OpenAI's `gpt-4o-mini` (released [today](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/)).\n",
        "\n",
        "The basic idea here is straightforward enough:\n",
        "\n",
        "1. We look at a document\n",
        "2. We generate questions that could be answered by that node\n",
        "\n",
        "This gives us a number of question/context pairs that we can use to fine-tune our Embeddings model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_EWfmIscMrvg"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "qa_chat_model = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-hLnsSB6Y-S"
      },
      "source": [
        "We'll create a simple Question Generation prompt to query `gpt-4o-mini` to generate Questions for each retrieved context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "diEWcw00NMSj"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "qa_prompt = \"\"\"\\\n",
        "Given the following context, you must generate questions based on only the provided context.\n",
        "\n",
        "You are to generate {n_questions} questions which should be provided in the following format:\n",
        "\n",
        "1. QUESTION #1\n",
        "2. QUESTION #2\n",
        "...\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "qa_prompt_template = ChatPromptTemplate.from_template(qa_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u87Izpgm6_fk"
      },
      "source": [
        "We'll create a simple chain to query the LLM!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ggl9SSjiNbpG"
      },
      "outputs": [],
      "source": [
        "question_generation_chain = qa_prompt_template | qa_chat_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4duvHirh7DQv"
      },
      "source": [
        "There's a lot going on in this function - let's take a deeper look:\n",
        "\n",
        "1. First, we provide a list of documents and a number of questions\n",
        "2. We, for each document in our list, generate `n_questions` of questions.\n",
        "3. We then associate those questions and contexts via a `UUID`.\n",
        "\n",
        "> NOTE: The reason we're doing this `UUID` association is for ease of use later in the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Lm2JvgC9X37"
      },
      "source": [
        "##### 🏗️ Activity #1:\n",
        "\n",
        "We have:\n",
        "\n",
        "- Lists of `Documents` with the `metadata` field `id`.\n",
        "\n",
        "We need:\n",
        "\n",
        "- An object with key `id`, which have values `str` questions.\n",
        "- An object with key `question_id`, which have values `List(str)` which will be a list of associated `context_id`.\n",
        "\n",
        "An Example:\n",
        "\n",
        "question_object:\n",
        "```python\n",
        "{\n",
        "'b4b95fb6-f827-4454-aa5b-20e62733f172': 'What types of accessible formats are available for persons with disabilities?',\n",
        "'df58ee4f-714c-419e-8324-94e5870574e2': 'How do accessible formats benefit persons with disabilities?',\n",
        "'505fce8b-0e56-48de-a251-61027e396918': 'What are some of the risks associated with the increasing capabilities of AI systems that generate synthetic content?',\n",
        "'8ff0ab33-60dc-4fee-8958-91bfb686aca8': 'Why is it important for providers of AI systems to embed technical solutions for marking and detecting synthetic content?'\n",
        "}\n",
        " ```\n",
        "\n",
        " context_object:\n",
        " ```python\n",
        "{\n",
        "'b4b95fb6-f827-4454-aa5b-20e62733f172': ['dd75bf94-75f3-4603-8e4b-5522f6925638'],\n",
        "'df58ee4f-714c-419e-8324-94e5870574e2': ['dd75bf94-75f3-4603-8e4b-5522f6925638'],\n",
        "'505fce8b-0e56-48de-a251-61027e396918': ['ffe3893f-688c-48e8-90bd-7a9feb953d90'],\n",
        "'8ff0ab33-60dc-4fee-8958-91bfb686aca8': ['ffe3893f-688c-48e8-90bd-7a9feb953d90'],\n",
        "}\n",
        " ```\n",
        "\n",
        " As you can see, a piece of context can be associated with more than 1 question.\n",
        "\n",
        " The task is to write the Python function(s) to accomplish this task.\n",
        "\n",
        " Your function signature is provided below, along with the desired return values.\n",
        "\n",
        " > NOTE: You can make any modifications that you desire - assuming that you have the correct input and outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "U4yi4NfTCnLc"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "import re\n",
        "\n",
        "async def create_questions(documents, n_questions):\n",
        "  questions = {}\n",
        "  relevant_docs = {}\n",
        "\n",
        "  ### YOUR CODE HERE\n",
        "  # Process each document to generate questions\n",
        "  for doc in tqdm.tqdm(documents, desc=\"Processing documents\"):\n",
        "        # Generate n_questions for each document using the question generation chain\n",
        "        generated_questions = question_generation_chain.invoke({\n",
        "            \"context\": doc.page_content,\n",
        "            \"n_questions\": n_questions\n",
        "        })\n",
        "\n",
        "        # For each generated question\n",
        "        for question in generated_questions.content.split('\\n'):\n",
        "            #remove the question number at the begining\n",
        "            question = re.sub(r\"(^\\d+. )\", \"\", question)\n",
        "            \n",
        "            # Generate a unique ID for the question\n",
        "            question_id = str(uuid.uuid4())\n",
        "\n",
        "            # Add to questions dictionary\n",
        "            questions[question_id] = question.strip()\n",
        "                \n",
        "            # Add to relevant docs dictionary - link question to source document\n",
        "            relevant_docs[question_id] = [doc.metadata[\"id\"]]\n",
        "              \n",
        "  return questions, relevant_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5W0eWOUo4QGL"
      },
      "source": [
        "### REMOVE `await` IF NOT USING ASYNC (HINT: Use `async`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85Dq6KRqEs0F",
        "outputId": "60dcd580-b6e8-4e3b-d605-05b492ca5c96"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing documents: 100%|██████████| 78/78 [01:29<00:00,  1.15s/it]\n"
          ]
        }
      ],
      "source": [
        "training_questions, training_relevant_contexts = await create_questions(training_split_documents, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FSTG0bb7w73"
      },
      "source": [
        "We'll use the function to generate training, validation, and test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIZm4CqGVzBx",
        "outputId": "65a7703a-c528-40f6-aff4-1be84902cfc4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing documents: 100%|██████████| 12/12 [00:18<00:00,  1.52s/it]\n"
          ]
        }
      ],
      "source": [
        "val_questions, val_relevant_contexts = await create_questions(val_split_documents, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6qUHg9sV2_y",
        "outputId": "b03bf5c6-d392-40bf-a061-1daceba2962e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing documents: 100%|██████████| 12/12 [00:13<00:00,  1.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'e766257a-5620-4650-8510-36bd822f7741': 'What are some potential societal impacts of the technology mentioned in the context?  ', '9aa330aa-32d7-49c3-b43e-0ab7d73ea8ac': 'Why does the author believe that the knowledge gap between tech enthusiasts and the general population is unhealthy?', '3a2a223d-5f76-42ed-9156-6a528f5128c0': 'What are some reasons people dislike LLMs according to the context?', 'a86fd938-aa11-4500-b0ca-5e699075feb5': 'Why is it important to discuss the criticisms of LLMs?', '1001c601-96e3-488b-bc49-838666f7c584': 'What are some potential consequences of making decisions based on hype and misinformation?  ', 'a449a292-7d6f-4390-bbb3-b0709d4a4f91': 'Why is it important to acknowledge good applications of certain tools before making decisions about their use?', '974ba56d-26a0-447e-8cc3-1fa236345123': \"What is the author's perspective on the environmental impact of plagiarism machines in the field discussed?  \", '94aa9911-47b2-40ce-8e63-0e8483e436a1': 'What does the author believe is necessary to help others understand the value in this field?', '9a4d9e05-c78f-4de8-90e3-f24212fb9ba9': 'What significant event regarding the term \"Artificial Intelligence\" occurred on January 7th?  ', 'a85576f3-e439-4348-ad94-b957ae336930': 'What was discussed on March 5th regarding prompt injection and jailbreaking?', '9b5376e5-900c-49ce-a345-939358334cbb': 'What is the focus of the event scheduled for June 21st regarding AI applications?  ', 'f1987496-c82e-4f58-b234-909ed184df10': 'What does the term \"Slop\" refer to in the context of AI-generated content as mentioned on May 8th?', '5cfdf963-60eb-4ec0-bc4a-9c2af9e18eb9': \"What new feature was added to Claude's API on August 23rd?\", '18b87be5-d098-4244-8994-103b8ac364e4': 'What was discussed in the weeknotes on September 12th regarding OpenAI?', '76a3efdf-30f6-4566-ad0d-31fc1ef186d3': 'What are the key features of NotebookLM’s automatically generated podcasts that make them effective?  ', 'ad5e584d-399c-4f92-8611-d64914d259c5': 'What topics were covered during OpenAI DevDay 2024?', '78766645-cd92-460d-8c3f-37d5f9d35965': 'What is the purpose of the new Claude analysis JavaScript code execution tool mentioned on the 24th?  ', '56b33db5-7cce-4b58-8149-a634113e227c': 'How can users interact with images, audio, and video in their terminal as noted on the 29th?', 'f99f2d2e-a8bb-4fe0-984f-521ef1419235': 'What advancements were made in running GPT-4 class models on personal laptops?', 'f3e6ee77-113a-4a5e-a5c1-4d8f39b502d0': \"How does ChatGPT Canvas's ability to make API requests compare to previous functionalities?\", '3ef0823d-fb66-465c-a161-32e75aa0e4da': 'What is the title of the article posted by Simon Willison on 31st December 2024?', '63edfddb-2d2f-4eea-b225-1a3e91a62454': 'When was the article \"Stuff we figured out about AI in 2023\" published?', '47260ad3-121f-4603-9aae-a3f6ef7ea6a8': 'What is the significance of the number 940 in the context of generative AI?  ', 'b4ef8d52-e66a-4a43-b259-08adb3e878b5': 'How does the term \"long-context\" relate to the other topics mentioned in the context?'}\n",
            "{'e766257a-5620-4650-8510-36bd822f7741': ['ef3ef8a5-5b95-4521-930d-27341dadec3d'], '9aa330aa-32d7-49c3-b43e-0ab7d73ea8ac': ['ef3ef8a5-5b95-4521-930d-27341dadec3d'], '3a2a223d-5f76-42ed-9156-6a528f5128c0': ['36530b86-5f20-4a35-ba17-946985243f01'], 'a86fd938-aa11-4500-b0ca-5e699075feb5': ['36530b86-5f20-4a35-ba17-946985243f01'], '1001c601-96e3-488b-bc49-838666f7c584': ['5b5e3831-8055-463f-981f-54b1e53e7097'], 'a449a292-7d6f-4390-bbb3-b0709d4a4f91': ['5b5e3831-8055-463f-981f-54b1e53e7097'], '974ba56d-26a0-447e-8cc3-1fa236345123': ['190c6977-f035-4cad-aaf9-a72223fcea89'], '94aa9911-47b2-40ce-8e63-0e8483e436a1': ['190c6977-f035-4cad-aaf9-a72223fcea89'], '9a4d9e05-c78f-4de8-90e3-f24212fb9ba9': ['da99332c-fd54-43c1-abc2-d2836362a646'], 'a85576f3-e439-4348-ad94-b957ae336930': ['da99332c-fd54-43c1-abc2-d2836362a646'], '9b5376e5-900c-49ce-a345-939358334cbb': ['d0b51b54-9248-40f7-a5ca-357d1c1bb1ca'], 'f1987496-c82e-4f58-b234-909ed184df10': ['d0b51b54-9248-40f7-a5ca-357d1c1bb1ca'], '5cfdf963-60eb-4ec0-bc4a-9c2af9e18eb9': ['1ce1c593-f927-4022-ba1f-dba19d2732ad'], '18b87be5-d098-4244-8994-103b8ac364e4': ['1ce1c593-f927-4022-ba1f-dba19d2732ad'], '76a3efdf-30f6-4566-ad0d-31fc1ef186d3': ['d881acc9-bfb7-4b88-8599-83c2a1052a88'], 'ad5e584d-399c-4f92-8611-d64914d259c5': ['d881acc9-bfb7-4b88-8599-83c2a1052a88'], '78766645-cd92-460d-8c3f-37d5f9d35965': ['884b7dd2-7ed2-4589-9a21-345a029dc688'], '56b33db5-7cce-4b58-8149-a634113e227c': ['884b7dd2-7ed2-4589-9a21-345a029dc688'], 'f99f2d2e-a8bb-4fe0-984f-521ef1419235': ['7904da2a-d282-4b01-a5b2-544a066db9c9'], 'f3e6ee77-113a-4a5e-a5c1-4d8f39b502d0': ['7904da2a-d282-4b01-a5b2-544a066db9c9'], '3ef0823d-fb66-465c-a161-32e75aa0e4da': ['c767ee3e-2edd-4604-bb8a-7dac00d59397'], '63edfddb-2d2f-4eea-b225-1a3e91a62454': ['c767ee3e-2edd-4604-bb8a-7dac00d59397'], '47260ad3-121f-4603-9aae-a3f6ef7ea6a8': ['648e572b-cc56-4ab3-95c2-5189d5614f5a'], 'b4ef8d52-e66a-4a43-b259-08adb3e878b5': ['648e572b-cc56-4ab3-95c2-5189d5614f5a']}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "test_questions, test_relevant_contexts = await create_questions(test_split_documents, 2)\n",
        "print(test_questions)\n",
        "print(test_relevant_contexts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "e766257a-5620-4650-8510-36bd822f7741 : What are some potential societal impacts of the technology mentioned in the context?  \n",
            "9aa330aa-32d7-49c3-b43e-0ab7d73ea8ac : Why does the author believe that the knowledge gap between tech enthusiasts and the general population is unhealthy?\n",
            "3a2a223d-5f76-42ed-9156-6a528f5128c0 : What are some reasons people dislike LLMs according to the context?\n",
            "a86fd938-aa11-4500-b0ca-5e699075feb5 : Why is it important to discuss the criticisms of LLMs?\n",
            "1001c601-96e3-488b-bc49-838666f7c584 : What are some potential consequences of making decisions based on hype and misinformation?  \n",
            "a449a292-7d6f-4390-bbb3-b0709d4a4f91 : Why is it important to acknowledge good applications of certain tools before making decisions about their use?\n",
            "974ba56d-26a0-447e-8cc3-1fa236345123 : What is the author's perspective on the environmental impact of plagiarism machines in the field discussed?  \n",
            "94aa9911-47b2-40ce-8e63-0e8483e436a1 : What does the author believe is necessary to help others understand the value in this field?\n",
            "9a4d9e05-c78f-4de8-90e3-f24212fb9ba9 : What significant event regarding the term \"Artificial Intelligence\" occurred on January 7th?  \n",
            "a85576f3-e439-4348-ad94-b957ae336930 : What was discussed on March 5th regarding prompt injection and jailbreaking?\n",
            "9b5376e5-900c-49ce-a345-939358334cbb : What is the focus of the event scheduled for June 21st regarding AI applications?  \n",
            "f1987496-c82e-4f58-b234-909ed184df10 : What does the term \"Slop\" refer to in the context of AI-generated content as mentioned on May 8th?\n",
            "5cfdf963-60eb-4ec0-bc4a-9c2af9e18eb9 : What new feature was added to Claude's API on August 23rd?\n",
            "18b87be5-d098-4244-8994-103b8ac364e4 : What was discussed in the weeknotes on September 12th regarding OpenAI?\n",
            "76a3efdf-30f6-4566-ad0d-31fc1ef186d3 : What are the key features of NotebookLM’s automatically generated podcasts that make them effective?  \n",
            "ad5e584d-399c-4f92-8611-d64914d259c5 : What topics were covered during OpenAI DevDay 2024?\n",
            "78766645-cd92-460d-8c3f-37d5f9d35965 : What is the purpose of the new Claude analysis JavaScript code execution tool mentioned on the 24th?  \n",
            "56b33db5-7cce-4b58-8149-a634113e227c : How can users interact with images, audio, and video in their terminal as noted on the 29th?\n",
            "f99f2d2e-a8bb-4fe0-984f-521ef1419235 : What advancements were made in running GPT-4 class models on personal laptops?\n",
            "f3e6ee77-113a-4a5e-a5c1-4d8f39b502d0 : How does ChatGPT Canvas's ability to make API requests compare to previous functionalities?\n",
            "3ef0823d-fb66-465c-a161-32e75aa0e4da : What is the title of the article posted by Simon Willison on 31st December 2024?\n",
            "63edfddb-2d2f-4eea-b225-1a3e91a62454 : When was the article \"Stuff we figured out about AI in 2023\" published?\n",
            "47260ad3-121f-4603-9aae-a3f6ef7ea6a8 : What is the significance of the number 940 in the context of generative AI?  \n",
            "b4ef8d52-e66a-4a43-b259-08adb3e878b5 : How does the term \"long-context\" relate to the other topics mentioned in the context?\n"
          ]
        }
      ],
      "source": [
        "for query_id, query in test_questions.items():\n",
        "    print(f\"{query_id} : {query}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_jYOnAI43zK"
      },
      "source": [
        "### Reformating and Saving Datasets\n",
        "\n",
        "Now, we can save our datasets for later use!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "iF6IFFq9VsNu"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "training_corpus = {train_item.metadata[\"id\"] : train_item.page_content for train_item in training_split_documents}\n",
        "\n",
        "train_dataset = {\n",
        "    \"questions\" : training_questions,\n",
        "    \"relevant_contexts\" : training_relevant_contexts,\n",
        "    \"corpus\" : training_corpus\n",
        "}\n",
        "\n",
        "with open(\"training_dataset.jsonl\", \"w\") as f:\n",
        "  json.dump(train_dataset, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "PqF9WaueV-V8"
      },
      "outputs": [],
      "source": [
        "val_corpus = {val_item.metadata[\"id\"] : val_item.page_content for val_item in val_split_documents}\n",
        "\n",
        "val_dataset = {\n",
        "    \"questions\" : val_questions,\n",
        "    \"relevant_contexts\" : val_relevant_contexts,\n",
        "    \"corpus\" : val_corpus\n",
        "}\n",
        "\n",
        "with open(\"val_dataset.jsonl\", \"w\") as f:\n",
        "  json.dump(val_dataset, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "0DSQ7WMnWAu6"
      },
      "outputs": [],
      "source": [
        "train_corpus = {test_item.metadata[\"id\"] : test_item.page_content for test_item in test_split_documents}\n",
        "\n",
        "test_dataset = {\n",
        "    \"questions\" : test_questions,\n",
        "    \"relevant_contexts\" : test_relevant_contexts,\n",
        "    \"corpus\" : train_corpus\n",
        "}\n",
        "\n",
        "with open(\"test_dataset.jsonl\", \"w\") as f:\n",
        "  json.dump(test_dataset, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAwklqQCgVi-"
      },
      "source": [
        "## Task 4: Fine-tuning `snowflake-arctic-embed-l`\n",
        "\n",
        "Now that we have a dataset, let's grab a `sentence-transformers` Embeddings model!\n",
        "\n",
        "We'll be using Snowflake's [`snowflake-arctic-embed-l`](https://huggingface.co/Snowflake/snowflake-arctic-embed-l) as a base embeddings model.\n",
        "\n",
        "It is a well performing embeddings model by itself, but there's a lot of very specific domain terms and vocabulary in our courpus - so lets fine-tune it and see what that can do for us!\n",
        "\n",
        ">> NOTE: Skip installing dependencies if you are running this notebook locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXzVHP3v1Cno",
        "outputId": "a9d6ca65-d355-460d-de89-7446a441512b"
      },
      "outputs": [],
      "source": [
        "!uv pip install -qU sentence_transformers datasets pyarrow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-PGsQB7Xo6V",
        "outputId": "8df58392-a82b-45f9-ce4e-b4155522e2c6"
      },
      "outputs": [],
      "source": [
        "#SentenceTransformer loads the pre-trained embedding model\n",
        "#This model will be used to generate embeddings for text queries and documents.\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model_id = \"Snowflake/snowflake-arctic-embed-l\"\n",
        "model = SentenceTransformer(model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ztG07iB8CFO"
      },
      "source": [
        "We'll grab some necessary imports from `sentence_transformers` and `torch`.\n",
        "\n",
        "> NOTE: PyTorch (`torch`) is a popular machine learning library - while we don't go very deep into PyTorch it's an incredibly powerful and interesting library! Please read more about it [here](https://pytorch.org/tutorials/beginner/basics/intro.html)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "B-WbpuUWYFJr"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from sentence_transformers import InputExample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJtPPlck8HBE"
      },
      "source": [
        "We're using a toy batch size here to reflect the limited number of examples we have.\n",
        "\n",
        "> NOTE: It is typical to use a much larger batch size (~64+), hardware permitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "8Lokhy6KYHAv"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-6DT8hc8PmT"
      },
      "source": [
        "Let's move our dataset into the expected format for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "JJk37zQsYJ4P"
      },
      "outputs": [],
      "source": [
        "#Prepare the Training Data\n",
        "corpus = train_dataset['corpus']\n",
        "queries = train_dataset['questions']\n",
        "relevant_docs = train_dataset['relevant_contexts']\n",
        "\n",
        "examples = []\n",
        "for query_id, query in queries.items():\n",
        "    doc_id = relevant_docs[query_id][0]\n",
        "    text = corpus[doc_id]\n",
        "    example = InputExample(texts=[query, text])\n",
        "    examples.append(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjFx7KHI8TL0"
      },
      "source": [
        "Now we can create a `torch` `DataLoader`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "tiizmeIqZ_-w"
      },
      "outputs": [],
      "source": [
        "loader = DataLoader(\n",
        "    examples, batch_size=BATCH_SIZE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vA8rzlX8XbT"
      },
      "source": [
        "Next up, we'll prepare our loss function!\n",
        "\n",
        "Loss is an important part of training, fine-tuning, and more. If you want a deep dive on loss - you can check out our [event on loss!](https://www.youtube.com/watch?v=iB8FWR9aD5Q&t=8s).\n",
        "\n",
        "The core loss we're using today is called `MultipleNegativesRankingLoss` - you can find more information [here](https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/losses/MultipleNegativesRankingLoss.py).\n",
        "\n",
        "This is \"wrapped\" in `MatryoshkaLoss`, which you can read the implementation of [here](https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/losses/MatryoshkaLoss.py)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Uga4nnBqlVeh"
      },
      "outputs": [],
      "source": [
        "#Defines a two-step loss function:\n",
        "#MultipleNegativesRankingLoss:\n",
        "#Encourages the model to bring queries closer to their correct documents while pushing away unrelated ones.\n",
        "#MatryoshkaLoss:\n",
        "#Ensures that embeddings retain meaning even when reduced to lower dimensions (useful for efficient storage & fast inference).\n",
        "from sentence_transformers.losses import MatryoshkaLoss, MultipleNegativesRankingLoss\n",
        "\n",
        "matryoshka_dimensions = [768, 512, 256, 128, 64]\n",
        "inner_train_loss = MultipleNegativesRankingLoss(model)\n",
        "train_loss = MatryoshkaLoss(\n",
        "    model, inner_train_loss, matryoshka_dims=matryoshka_dimensions\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJG4fOm66PHI"
      },
      "source": [
        "##### 🏗️ Activity #2:\n",
        "\n",
        "Both of these losses sound \"cool\", but what are they - exactly - under the hood?\n",
        "\n",
        "Why are these losses specifically doing? Please write a short summary of each loss.\n",
        "\n",
        "> NOTE: This is a course focused on AI Engineering and the application of AI - looking for a hint? Try pasting the code (linked above) into ChatGPT/Claude to write the summary!\n",
        "\n",
        "**ANSWER**\n",
        "\n",
        "**MultipleNegativesRankingLoss** helps train sentence embeddings so that:\n",
        "\n",
        "The correct (positive) sentence is placed closer to the query in the embedding space. Other (negative) sentences are pushed further away.\n",
        "Instead of manually selecting \"wrong\" answers (negatives), it automatically treats all other sentences in the batch as negatives—this is what makes it so efficient.\n",
        "\n",
        "🛠 How Does It Work?\n",
        "- Input Setup:\n",
        "\n",
        "You have a batch of queries and their correct answers (positives).\n",
        "Each pair consists of (Query, Correct Answer).\n",
        "\n",
        "- Computing Similarities:\n",
        "\n",
        "The model generates embeddings (numerical representations) for all queries and answers.\n",
        "It then compares each query with every answer in the batch using cosine similarity.\n",
        "\n",
        "- Ranking the Correct Answer Higher:\n",
        "\n",
        "The loss function rewards the model when the similarity between the query and its correct answer is high.\n",
        "It penalizes the model when the query mistakenly ranks other answers higher.\n",
        "\n",
        "- In-Batch Negatives Trick:\n",
        "\n",
        "Instead of needing separate wrong answers, the model automatically considers every other answer in the batch as a negative.\n",
        "This makes training much faster and more efficient.\n",
        "\n",
        "🤔 Why is it Useful?\n",
        "\n",
        "✅ Efficient Learning – No need to manually pick negative samples.\n",
        "\n",
        "✅ Better Retrieval Models – Helps the model understand which sentences are most relevant.\n",
        "\n",
        "✅ Works Well in Large Batches – The more sentences in a batch, the more negatives, improving performance\n",
        "\n",
        "--------------------\n",
        "\n",
        "**MatryoshkaLoss** is a loss function that trains embeddings to be useful at multiple dimensions (sizes) at the same time.\n",
        "\n",
        "1️⃣ Encourages embeddings to store important meaning at the start of the vector.\n",
        "\n",
        "2️⃣ Makes sure truncated embeddings (shorter versions) still work well.\n",
        "\n",
        "3️⃣ Allows models to be flexible—you can use smaller embeddings for speed while keeping good performance.\n",
        "\n",
        "🛠 How Does It Work?\n",
        "- Start with a high-dimensional embedding (e.g., 768 dimensions).\n",
        "- Create smaller versions by chopping off some of the last dimensions (e.g., reduce to 512, 256, 128, 64).\n",
        "- Train each version separately using a ranking loss (e.g., MultipleNegativesRankingLoss).\n",
        "- Ensure that smaller versions still perform well—this forces the model to store the most important meaning in the first few dimensions.\n",
        "\n",
        "🤔 Why Is This Useful?\n",
        "\n",
        "✅ More flexibility – You can use smaller embeddings if needed without losing too much quality.\n",
        "\n",
        "✅ Faster inference – If you need speed, use a lower-dimension version.\n",
        "\n",
        "✅ Efficient storage – Store compact embeddings while still capturing important meaning.\n",
        "\n",
        "✅ Works well for retrieval & search – Smaller, optimized embeddings can make search systems faster.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKxRuXfH844c"
      },
      "source": [
        "Now we can set-up our evaluator.\n",
        "\n",
        "> NOTE: Due to the formatting of our dataset - this is all we have to do!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "f0hAFwUyaHQG"
      },
      "outputs": [],
      "source": [
        "#Loads validation data from val_dataset.\n",
        "#Creates an InformationRetrievalEvaluator, which:\n",
        "#Measures how well the model ranks relevant documents for given queries.\n",
        "#Uses metrics like Mean Average Precision (MAP) (How well the model ranks relevant documents across queries.)\n",
        "# and Recall@K. (Measures if the correct document appears in the top K results.)\n",
        "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
        "\n",
        "corpus = val_dataset['corpus']\n",
        "queries = val_dataset['questions']\n",
        "relevant_docs = val_dataset['relevant_contexts']\n",
        "\n",
        "evaluator = InformationRetrievalEvaluator(queries, corpus, relevant_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYfap_ct8-bU"
      },
      "source": [
        "We'll train this model for 5 epochs, though you could increase this number if we had a significant amount more data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "svZG0pBHiQr6"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxitWoNX9DwW"
      },
      "source": [
        "It's training time!\n",
        "\n",
        "> NOTE: We're manually defining a warm-up period here - this is just to provide a smooth ramp into our training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "!uv pip install -qU wandb 'accelerate>=0.26.0' 'ipywidgets>=8.1.5'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dummy/dummy/runs/kcpedsqf?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x177f7ca10>"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Disables Weights & Biases (wandb) tracking for this experiment.\n",
        "#Useful when debugging or running tests without logging.\n",
        "\n",
        "import wandb\n",
        "wandb.init(mode=\"disabled\") # Completely disables logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 753,
          "referenced_widgets": [
            "0d3fc6edfdab4fe9aff8e805632eadad",
            "ebe8aa7a82124b57bce2d826c1dea0fa",
            "aa11c10b4234452d9430744ab89b59a2",
            "ca05cbcd72cb41d9856804ffb17b26cb",
            "cc2a33e9a7ac4c5699346fcbf53b7c95",
            "408f4dfce21a45cfad36047677ec8658",
            "ca5804644ef345c1b4f670fb7f088fe8",
            "2a872283afaa4a33a9bc3f9e57b3650b",
            "fe4d1052824c4ed29c38c5311087e650",
            "e283b1608c4d4266a03c57dc95aabb2e",
            "bfc4997e3bd94e66bebaa1ffdae1b99e"
          ]
        },
        "id": "aDhUHZY-iR09",
        "outputId": "6dbd9320-f7b9-46d6-b891-efdd12086631"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7420ecb47f2746cf9dcf648515ef5c2b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [160/160 04:55, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Cosine Accuracy@1</th>\n",
              "      <th>Cosine Accuracy@3</th>\n",
              "      <th>Cosine Accuracy@5</th>\n",
              "      <th>Cosine Accuracy@10</th>\n",
              "      <th>Cosine Precision@1</th>\n",
              "      <th>Cosine Precision@3</th>\n",
              "      <th>Cosine Precision@5</th>\n",
              "      <th>Cosine Precision@10</th>\n",
              "      <th>Cosine Recall@1</th>\n",
              "      <th>Cosine Recall@3</th>\n",
              "      <th>Cosine Recall@5</th>\n",
              "      <th>Cosine Recall@10</th>\n",
              "      <th>Cosine Ndcg@10</th>\n",
              "      <th>Cosine Mrr@10</th>\n",
              "      <th>Cosine Map@100</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.984622</td>\n",
              "      <td>0.979167</td>\n",
              "      <td>0.979167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.969244</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.958333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.969244</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.958333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.969244</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.958333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.979167</td>\n",
              "      <td>0.972222</td>\n",
              "      <td>0.972222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.979167</td>\n",
              "      <td>0.972222</td>\n",
              "      <td>0.972222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.963789</td>\n",
              "      <td>0.951389</td>\n",
              "      <td>0.951389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.979167</td>\n",
              "      <td>0.972222</td>\n",
              "      <td>0.972222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.979167</td>\n",
              "      <td>0.972222</td>\n",
              "      <td>0.972222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.319444</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.976278</td>\n",
              "      <td>0.968750</td>\n",
              "      <td>0.968750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.319444</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.976278</td>\n",
              "      <td>0.968750</td>\n",
              "      <td>0.968750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.319444</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.976278</td>\n",
              "      <td>0.968750</td>\n",
              "      <td>0.968750</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Warm-up steps prevent sudden learning rate spikes.\n",
        "#Formula:\n",
        "#len(loader) * EPOCHS = total number of training steps.\n",
        "#0.1 = 10% of steps used for gradual learning rate increase.\n",
        "warmup_steps = int(len(loader) * EPOCHS * 0.1)\n",
        "\n",
        "#Trains the model using the train_loss function.\n",
        "#Fine-tunes it over multiple epochs to improve retrieval accuracy.\n",
        "#Evaluates performance every 50 steps using the evaluator.\n",
        "#Saves the fine-tuned model to 'finetuned_arctic_ft'.\n",
        "model.fit(\n",
        "    train_objectives=[(loader, train_loss)],\n",
        "    epochs=EPOCHS,\n",
        "    warmup_steps=warmup_steps,\n",
        "    output_path='finetuned_arctic_ft',\n",
        "    show_progress_bar=True,\n",
        "    evaluator=evaluator,\n",
        "    evaluation_steps=50\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "58699484312f460cb96ac44e3af14aa2",
            "d45a7d95e5b14a6f88d5798fec9c40a7",
            "b011a6ccf8e745c6be6f3a17b7d61dce",
            "1016780729b04ba489d488922173ddae",
            "9d34dac405fd40139d5dc04eecef55ed",
            "d6169577ffb341a69eb0175e301b6a44",
            "d4acc9a7aca04564bfaefe33de079394",
            "4e2c257232c3472697e03350de43cb30",
            "381c780ce17e4662981175400fb8a0f8",
            "878dc96f87dd45f389948a546db33e94",
            "006bf6f5ebaf400486a6b82610381db0",
            "9198dd0fa8f04aa7b75307aa2d513bfb",
            "59cd26ae53024fbd85431b6683cd119c",
            "4e7ccd97042c4fb9a201b6dc76762e04",
            "72ec09e2cbbf4788b1561abfcdd0819a",
            "fb1e19624fde4d3c8048ce00264d6056",
            "9638a0456e0c41b1b9b9757932f55c53",
            "18825dc83221412ab602830fc00db71b",
            "a6ea48a80c194d128959422368aa0e10",
            "9f4026c62c60493caa18c014ae414e65"
          ]
        },
        "id": "b3iwclvyRD8L",
        "outputId": "1471e984-9351-478c-de34-6eb32263fa30"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "521213e6418a46f7924973829cada1c4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "_pn-Y6yjRoHk"
      },
      "outputs": [],
      "source": [
        "hf_username = \"kcheng0816\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "e5c86e0e33264ed8b6325e44f9421653",
            "acf3991e5d644f468264f561b28b514a",
            "4a723c6a56e94309b2e3abe63f28aedc",
            "ef25768d3b0746b48c6d02e9bd151bf9",
            "cc1f0bc4a1a74568b9c74a7392a1568f",
            "95160a05de5b402b9bdb0bd2d099cd00",
            "cf376b0ea3544055b8867d7013526502",
            "869814ecd49e46f9a33dd53130e6953f",
            "cc7d460d3c5a4ac6a9b64929736d6598",
            "13e9bf583f6442d395334947379a280a",
            "1995b4d98fe044e38f1980d47033ca40"
          ]
        },
        "id": "Nqhf3zWa9AiJ",
        "outputId": "c601b2a8-f8e9-4d71-9c7b-8ea4999ff077"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eacb219ee3ab484e9d771edc242ad46f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "'https://huggingface.co/kcheng0816/aie5_assignment8/commit/c287feaa2b7d4b87c3d6b521117d43fabe5d4d09'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Uploads the fine-tuned model. Makes the model publicly available for reusability.\n",
        "model.push_to_hub(f\"{hf_username}/finetuned_arctic_kc\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bo0zW5k9Poq"
      },
      "source": [
        "## Task 5: Evaluating our Retriever\n",
        "\n",
        "Now that we have fine-tuned our retriever - let's see if it's worthwhile!\n",
        "\n",
        "We'll start with some basic imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Vq-2oqU0wHFr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "from langchain_core.documents import Document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jD0qrIh9X8f"
      },
      "source": [
        "Now we'll define a function that will help us evaluate our retrieval process.\n",
        "\n",
        "> NOTE: We're assuming 1 correct document in a \"hit\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "0713_3cowX4q"
      },
      "outputs": [],
      "source": [
        "#Creates a FAISS-based vector search index.\n",
        "#Converts the corpus into vector embeddings.\n",
        "#Retrieves top-K most similar documents for each query.\n",
        "#Checks if the correct document is in the retrieved results (is_hit flag).\n",
        "#Returns evaluation results (how often the correct document appears in the top-K results).\n",
        "def evaluate_openai(\n",
        "    dataset,\n",
        "    embed_model,\n",
        "    top_k=5,\n",
        "    verbose=False,\n",
        "):\n",
        "  corpus = dataset['corpus']\n",
        "  questions = dataset['questions']\n",
        "  relevant_docs = dataset['relevant_contexts']\n",
        "  documents = [Document(page_content=content, metadata={\"id\": doc_id}) for doc_id, content in corpus.items()]\n",
        "  vectorstore = FAISS.from_documents(documents, embed_model)\n",
        "\n",
        "  retriever = vectorstore.as_retriever(search_kwargs={\"k\": top_k})\n",
        "\n",
        "  eval_results = []\n",
        "  #print(questions)\n",
        "  for id, question in questions.items():\n",
        "    retrieved_nodes = retriever.invoke(question)\n",
        "    retrieved_ids = [node.metadata[\"id\"] for node in retrieved_nodes]\n",
        "    expected_id = relevant_docs[id][0]\n",
        "    is_hit = expected_id in retrieved_ids\n",
        "    eval_results.append({\"id\": id, \"question\": question, \"expected_id\": expected_id, \"is_hit\": is_hit})\n",
        "\n",
        "  return eval_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOr49m4O9lxY"
      },
      "source": [
        "All that's left to do is evaluate, we'll evaluate our model against:\n",
        "\n",
        "1. OpenAI's closed source `text-embedding-3-small`\n",
        "2. The base non-fine-tuned version of `Snowflake/snowflake-arctic-embed-l`.\n",
        "\n",
        "Let's see how it stacks up!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijaeYpf593IW"
      },
      "source": [
        "### `text-embedding-3-small`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "!uv pip install -qU faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'questions': {'e766257a-5620-4650-8510-36bd822f7741': 'What are some potential societal impacts of the technology mentioned in the context?  ', '9aa330aa-32d7-49c3-b43e-0ab7d73ea8ac': 'Why does the author believe that the knowledge gap between tech enthusiasts and the general population is unhealthy?', '3a2a223d-5f76-42ed-9156-6a528f5128c0': 'What are some reasons people dislike LLMs according to the context?', 'a86fd938-aa11-4500-b0ca-5e699075feb5': 'Why is it important to discuss the criticisms of LLMs?', '1001c601-96e3-488b-bc49-838666f7c584': 'What are some potential consequences of making decisions based on hype and misinformation?  ', 'a449a292-7d6f-4390-bbb3-b0709d4a4f91': 'Why is it important to acknowledge good applications of certain tools before making decisions about their use?', '974ba56d-26a0-447e-8cc3-1fa236345123': \"What is the author's perspective on the environmental impact of plagiarism machines in the field discussed?  \", '94aa9911-47b2-40ce-8e63-0e8483e436a1': 'What does the author believe is necessary to help others understand the value in this field?', '9a4d9e05-c78f-4de8-90e3-f24212fb9ba9': 'What significant event regarding the term \"Artificial Intelligence\" occurred on January 7th?  ', 'a85576f3-e439-4348-ad94-b957ae336930': 'What was discussed on March 5th regarding prompt injection and jailbreaking?', '9b5376e5-900c-49ce-a345-939358334cbb': 'What is the focus of the event scheduled for June 21st regarding AI applications?  ', 'f1987496-c82e-4f58-b234-909ed184df10': 'What does the term \"Slop\" refer to in the context of AI-generated content as mentioned on May 8th?', '5cfdf963-60eb-4ec0-bc4a-9c2af9e18eb9': \"What new feature was added to Claude's API on August 23rd?\", '18b87be5-d098-4244-8994-103b8ac364e4': 'What was discussed in the weeknotes on September 12th regarding OpenAI?', '76a3efdf-30f6-4566-ad0d-31fc1ef186d3': 'What are the key features of NotebookLM’s automatically generated podcasts that make them effective?  ', 'ad5e584d-399c-4f92-8611-d64914d259c5': 'What topics were covered during OpenAI DevDay 2024?', '78766645-cd92-460d-8c3f-37d5f9d35965': 'What is the purpose of the new Claude analysis JavaScript code execution tool mentioned on the 24th?  ', '56b33db5-7cce-4b58-8149-a634113e227c': 'How can users interact with images, audio, and video in their terminal as noted on the 29th?', 'f99f2d2e-a8bb-4fe0-984f-521ef1419235': 'What advancements were made in running GPT-4 class models on personal laptops?', 'f3e6ee77-113a-4a5e-a5c1-4d8f39b502d0': \"How does ChatGPT Canvas's ability to make API requests compare to previous functionalities?\", '3ef0823d-fb66-465c-a161-32e75aa0e4da': 'What is the title of the article posted by Simon Willison on 31st December 2024?', '63edfddb-2d2f-4eea-b225-1a3e91a62454': 'When was the article \"Stuff we figured out about AI in 2023\" published?', '47260ad3-121f-4603-9aae-a3f6ef7ea6a8': 'What is the significance of the number 940 in the context of generative AI?  ', 'b4ef8d52-e66a-4a43-b259-08adb3e878b5': 'How does the term \"long-context\" relate to the other topics mentioned in the context?'}, 'relevant_contexts': {'e766257a-5620-4650-8510-36bd822f7741': ['ef3ef8a5-5b95-4521-930d-27341dadec3d'], '9aa330aa-32d7-49c3-b43e-0ab7d73ea8ac': ['ef3ef8a5-5b95-4521-930d-27341dadec3d'], '3a2a223d-5f76-42ed-9156-6a528f5128c0': ['36530b86-5f20-4a35-ba17-946985243f01'], 'a86fd938-aa11-4500-b0ca-5e699075feb5': ['36530b86-5f20-4a35-ba17-946985243f01'], '1001c601-96e3-488b-bc49-838666f7c584': ['5b5e3831-8055-463f-981f-54b1e53e7097'], 'a449a292-7d6f-4390-bbb3-b0709d4a4f91': ['5b5e3831-8055-463f-981f-54b1e53e7097'], '974ba56d-26a0-447e-8cc3-1fa236345123': ['190c6977-f035-4cad-aaf9-a72223fcea89'], '94aa9911-47b2-40ce-8e63-0e8483e436a1': ['190c6977-f035-4cad-aaf9-a72223fcea89'], '9a4d9e05-c78f-4de8-90e3-f24212fb9ba9': ['da99332c-fd54-43c1-abc2-d2836362a646'], 'a85576f3-e439-4348-ad94-b957ae336930': ['da99332c-fd54-43c1-abc2-d2836362a646'], '9b5376e5-900c-49ce-a345-939358334cbb': ['d0b51b54-9248-40f7-a5ca-357d1c1bb1ca'], 'f1987496-c82e-4f58-b234-909ed184df10': ['d0b51b54-9248-40f7-a5ca-357d1c1bb1ca'], '5cfdf963-60eb-4ec0-bc4a-9c2af9e18eb9': ['1ce1c593-f927-4022-ba1f-dba19d2732ad'], '18b87be5-d098-4244-8994-103b8ac364e4': ['1ce1c593-f927-4022-ba1f-dba19d2732ad'], '76a3efdf-30f6-4566-ad0d-31fc1ef186d3': ['d881acc9-bfb7-4b88-8599-83c2a1052a88'], 'ad5e584d-399c-4f92-8611-d64914d259c5': ['d881acc9-bfb7-4b88-8599-83c2a1052a88'], '78766645-cd92-460d-8c3f-37d5f9d35965': ['884b7dd2-7ed2-4589-9a21-345a029dc688'], '56b33db5-7cce-4b58-8149-a634113e227c': ['884b7dd2-7ed2-4589-9a21-345a029dc688'], 'f99f2d2e-a8bb-4fe0-984f-521ef1419235': ['7904da2a-d282-4b01-a5b2-544a066db9c9'], 'f3e6ee77-113a-4a5e-a5c1-4d8f39b502d0': ['7904da2a-d282-4b01-a5b2-544a066db9c9'], '3ef0823d-fb66-465c-a161-32e75aa0e4da': ['c767ee3e-2edd-4604-bb8a-7dac00d59397'], '63edfddb-2d2f-4eea-b225-1a3e91a62454': ['c767ee3e-2edd-4604-bb8a-7dac00d59397'], '47260ad3-121f-4603-9aae-a3f6ef7ea6a8': ['648e572b-cc56-4ab3-95c2-5189d5614f5a'], 'b4ef8d52-e66a-4a43-b259-08adb3e878b5': ['648e572b-cc56-4ab3-95c2-5189d5614f5a']}, 'corpus': {'ef3ef8a5-5b95-4521-930d-27341dadec3d': 'The knowledge gap between the people who actively follow this stuff and the 99% of the population who do not is vast.\\nThe pace of change doesn’t help either. In just the past month we’ve seen general availability of live interfaces where you can point your phone’s camera at something and talk about it with your voice... and optionally have it pretend to be Santa. Most self-certified nerds haven’t even tried that yet.\\nGiven the ongoing (and potential) impact on society that this technology has, I don’t think the size of this gap is healthy. I’d like to see a lot more effort put into improving this.\\nLLMs need better criticism', '36530b86-5f20-4a35-ba17-946985243f01': 'A lot of people absolutely hate this stuff. In some of the spaces I hang out (Mastodon, Bluesky, Lobste.rs, even Hacker News on occasion) even suggesting that “LLMs are useful” can be enough to kick off a huge fight.\\nI get it. There are plenty of reasons to dislike this technology—the environmental impact, the (lack of) ethics of the training data, the lack of reliability, the negative applications, the potential impact on people’s jobs.\\nLLMs absolutely warrant criticism. We need to be talking through these problems, finding ways to mitigate them and helping people learn how to use these tools responsibly in ways where the positive applications outweigh the negative.', '5b5e3831-8055-463f-981f-54b1e53e7097': 'I like people who are skeptical of this stuff. The hype has been deafening for more than two years now, and there are enormous quantities of snake oil and misinformation out there. A lot of very bad decisions are being made based on that hype. Being critical is a virtue.\\nIf we want people with decision-making authority to make good decisions about how to apply these tools we first need to acknowledge that there ARE good applications, and then help explain how to put those into practice while avoiding the many unintiutive traps.\\n(If you still don’t think there are any good applications at all I’m not sure why you made it to this point in the article!)', '190c6977-f035-4cad-aaf9-a72223fcea89': 'I think telling people that this whole field is environmentally catastrophic plagiarism machines that constantly make things up is doing those people a disservice, no matter how much truth that represents. There is genuine value to be had here, but getting to that value is unintuitive and needs guidance.\\nThose of us who understand this stuff have a duty to help everyone else figure it out.\\nEverything tagged “llms” on my blog in 2024\\nBecause I undoubtedly missed a whole bunch of things, here’s every long-form post I wrote in 2024 that I tagged with llms:', 'da99332c-fd54-43c1-abc2-d2836362a646': 'January\\n\\n7th: It’s OK to call it Artificial Intelligence\\n\\n9th: What I should have said about the term Artificial Intelligence\\n\\n17th: Talking about Open Source LLMs on Oxide and Friends\\n\\n26th: LLM 0.13: The annotated release notes\\n\\n\\n\\nFebruary\\n\\n21st: The killer app of Gemini Pro 1.5 is video\\n\\n\\n\\nMarch\\n\\n5th: Prompt injection and jailbreaking are not the same thing\\n\\n8th: The GPT-4 barrier has finally been broken\\n\\n22nd: Claude and ChatGPT for ad-hoc sidequests\\n\\n23rd: Building and testing C extensions for SQLite with ChatGPT Code Interpreter\\n\\n26th: llm cmd undo last git commit—a new plugin for LLM\\n\\n\\n\\nApril\\n\\n8th: Building files-to-prompt entirely using Claude 3 Opus\\n\\n10th: Three major LLM releases in 24 hours (plus weeknotes)', 'd0b51b54-9248-40f7-a5ca-357d1c1bb1ca': '17th: AI for Data Journalism: demonstrating what we can do with this stuff right now\\n\\n22nd: Options for accessing Llama 3 from the terminal using LLM\\n\\n\\n\\nMay\\n\\n8th: Slop is the new name for unwanted AI-generated content\\n\\n15th: ChatGPT in “4o” mode is not running the new features yet\\n\\n29th: Training is not the same as chatting: ChatGPT and other LLMs don’t remember everything you say\\n\\n\\n\\nJune\\n\\n6th: Accidental prompt injection against RAG applications\\n\\n10th: Thoughts on the WWDC 2024 keynote on Apple Intelligence\\n\\n17th: Language models on the command-line\\n\\n21st: Building search-based RAG using Claude, Datasette and Val Town\\n\\n27th: Open challenges for AI engineering\\n\\n\\n\\nJuly\\n\\n14th: Imitation Intelligence, my keynote for PyCon US 2024', '1ce1c593-f927-4022-ba1f-dba19d2732ad': '19th: Weeknotes: GPT-4o mini, LLM 0.15, sqlite-utils 3.37 and building a staging environment\\n\\n\\n\\nAugust\\n\\n6th: Weeknotes: a staging environment, a Datasette alpha and a bunch of new LLMs\\n\\n8th: django-http-debug, a new Django app mostly written by Claude\\n\\n23rd: Claude’s API now supports CORS requests, enabling client-side applications\\n\\n26th: Building a tool showing how Gemini Pro can return bounding boxes for objects in images\\n\\n\\n\\nSeptember\\n\\n6th: Calling LLMs from client-side JavaScript, converting PDFs to HTML + weeknotes\\n\\n10th: Notes from my appearance on the Software Misadventures Podcast\\n\\n12th: Notes on OpenAI’s new o1 chain-of-thought models\\n\\n20th: Notes on using LLMs for code', 'd881acc9-bfb7-4b88-8599-83c2a1052a88': '29th: NotebookLM’s automatically generated podcasts are surprisingly effective\\n\\n30th: Weeknotes: Three podcasts, two trips and a new plugin system\\n\\n\\n\\nOctober\\n\\n1st: OpenAI DevDay 2024 live blog\\n\\n2nd: OpenAI DevDay: Let’s build developer tools, not digital God\\n\\n15th: ChatGPT will happily write you a thinly disguised horoscope\\n\\n17th: Video scraping: extracting JSON data from a 35 second screen capture for less than 1/10th of a cent\\n\\n18th: Experimenting with audio input and output for the OpenAI Chat Completion API\\n\\n19th: Running Llama 3.2 Vision and Phi-3.5 Vision on a Mac with mistral.rs\\n\\n21st: Everything I built with Claude Artifacts this week\\n\\n22nd: Initial explorations of Anthropic’s new Computer Use capability', '884b7dd2-7ed2-4589-9a21-345a029dc688': '24th: Notes on the new Claude analysis JavaScript code execution tool\\n\\n27th: Run a prompt to generate and execute jq programs using llm-jq\\n\\n29th: You can now run prompts against images, audio and video in your terminal using LLM\\n\\n30th: W̶e̶e̶k̶n̶o̶t̶e̶s̶  Monthnotes for October\\n\\n\\n\\nNovember\\n\\n4th: Claude 3.5 Haiku\\n\\n7th: Project: VERDAD—tracking misinformation in radio broadcasts using Gemini 1.5\\n\\n12th: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac\\n\\n19th: Notes from Bing Chat—Our First Encounter With Manipulative AI\\n\\n25th: Ask questions of SQLite databases and CSV/JSON files in your terminal\\n\\n\\n\\nDecember\\n\\n4th: First impressions of the new Amazon Nova LLMs (via a new llm-bedrock plugin)\\n\\n7th: Prompts.js', '7904da2a-d282-4b01-a5b2-544a066db9c9': '7th: Prompts.js\\n\\n9th: I can now run a GPT-4 class model on my laptop\\n\\n10th: ChatGPT Canvas can make API requests now, but it’s complicated\\n\\n11th: Gemini 2.0 Flash: An outstanding multi-modal LLM with a sci-fi streaming mode\\n\\n19th: Building Python tools with a one-shot prompt using uv run and Claude Projects\\n\\n19th: Gemini 2.0 Flash “Thinking mode”\\n\\n20th: December in LLMs has been a lot\\n\\n20th: Live blog: the 12th day of OpenAI—“Early evals for OpenAI o3”\\n\\n24th: Trying out QvQ—Qwen’s new visual reasoning model\\n\\n31st: Things we learned about LLMs in 2024\\n\\n\\n\\n\\n(This list generated using Django SQL Dashboard with a SQL query written for me by Claude.)', 'c767ee3e-2edd-4604-bb8a-7dac00d59397': \"Posted 31st December 2024 at 6:07 pm · Follow me on Mastodon or Twitter or subscribe to my newsletter\\n\\n\\nMore recent articles\\n\\nURL-addressable Pyodide Python environments - 13th February 2025\\nUsing pip to install a Large Language Model that's under 100MB - 7th February 2025\\nOpenAI o3-mini, now available in LLM - 31st January 2025\\n\\n\\n \\n\\n\\nThis is Things we learned about LLMs in 2024 by Simon Willison, posted on 31st December 2024.\\n\\nPart of series LLMs annual review\\n\\nStuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m. \\nThings we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m. \\n\\n\\n\\n            google\\n            346\\n\\n\\n            ai\\n            1096\\n\\n\\n            openai\\n            255\", '648e572b-cc56-4ab3-95c2-5189d5614f5a': \"generative-ai\\n            940\\n\\n\\n            llms\\n            928\\n\\n\\n            anthropic\\n            114\\n\\n\\n            gemini\\n            56\\n\\n\\n            meta\\n            26\\n\\n\\n            inference-scaling\\n            28\\n\\n\\n            long-context\\n            10\\n\\nNext: Ending a year long posting streak\\nPrevious: Trying out QvQ - Qwen's new visual reasoning model\\n\\n\\n \\n \\n\\n\\nColophon\\n©\\n2002\\n2003\\n2004\\n2005\\n2006\\n2007\\n2008\\n2009\\n2010\\n2011\\n2012\\n2013\\n2014\\n2015\\n2016\\n2017\\n2018\\n2019\\n2020\\n2021\\n2022\\n2023\\n2024\\n2025\"}}\n"
          ]
        }
      ],
      "source": [
        "print(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyY3PztaxnU3",
        "outputId": "5a1ec5e9-00fc-4140-d5b6-aa752dd4c9fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'e766257a-5620-4650-8510-36bd822f7741': 'What are some potential societal impacts of the technology mentioned in the context?  ', '9aa330aa-32d7-49c3-b43e-0ab7d73ea8ac': 'Why does the author believe that the knowledge gap between tech enthusiasts and the general population is unhealthy?', '3a2a223d-5f76-42ed-9156-6a528f5128c0': 'What are some reasons people dislike LLMs according to the context?', 'a86fd938-aa11-4500-b0ca-5e699075feb5': 'Why is it important to discuss the criticisms of LLMs?', '1001c601-96e3-488b-bc49-838666f7c584': 'What are some potential consequences of making decisions based on hype and misinformation?  ', 'a449a292-7d6f-4390-bbb3-b0709d4a4f91': 'Why is it important to acknowledge good applications of certain tools before making decisions about their use?', '974ba56d-26a0-447e-8cc3-1fa236345123': \"What is the author's perspective on the environmental impact of plagiarism machines in the field discussed?  \", '94aa9911-47b2-40ce-8e63-0e8483e436a1': 'What does the author believe is necessary to help others understand the value in this field?', '9a4d9e05-c78f-4de8-90e3-f24212fb9ba9': 'What significant event regarding the term \"Artificial Intelligence\" occurred on January 7th?  ', 'a85576f3-e439-4348-ad94-b957ae336930': 'What was discussed on March 5th regarding prompt injection and jailbreaking?', '9b5376e5-900c-49ce-a345-939358334cbb': 'What is the focus of the event scheduled for June 21st regarding AI applications?  ', 'f1987496-c82e-4f58-b234-909ed184df10': 'What does the term \"Slop\" refer to in the context of AI-generated content as mentioned on May 8th?', '5cfdf963-60eb-4ec0-bc4a-9c2af9e18eb9': \"What new feature was added to Claude's API on August 23rd?\", '18b87be5-d098-4244-8994-103b8ac364e4': 'What was discussed in the weeknotes on September 12th regarding OpenAI?', '76a3efdf-30f6-4566-ad0d-31fc1ef186d3': 'What are the key features of NotebookLM’s automatically generated podcasts that make them effective?  ', 'ad5e584d-399c-4f92-8611-d64914d259c5': 'What topics were covered during OpenAI DevDay 2024?', '78766645-cd92-460d-8c3f-37d5f9d35965': 'What is the purpose of the new Claude analysis JavaScript code execution tool mentioned on the 24th?  ', '56b33db5-7cce-4b58-8149-a634113e227c': 'How can users interact with images, audio, and video in their terminal as noted on the 29th?', 'f99f2d2e-a8bb-4fe0-984f-521ef1419235': 'What advancements were made in running GPT-4 class models on personal laptops?', 'f3e6ee77-113a-4a5e-a5c1-4d8f39b502d0': \"How does ChatGPT Canvas's ability to make API requests compare to previous functionalities?\", '3ef0823d-fb66-465c-a161-32e75aa0e4da': 'What is the title of the article posted by Simon Willison on 31st December 2024?', '63edfddb-2d2f-4eea-b225-1a3e91a62454': 'When was the article \"Stuff we figured out about AI in 2023\" published?', '47260ad3-121f-4603-9aae-a3f6ef7ea6a8': 'What is the significance of the number 940 in the context of generative AI?  ', 'b4ef8d52-e66a-4a43-b259-08adb3e878b5': 'How does the term \"long-context\" relate to the other topics mentioned in the context?'}\n"
          ]
        }
      ],
      "source": [
        "te3_openai = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "te3_results = evaluate_openai(test_dataset, te3_openai)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "kkyW90TCxx_i"
      },
      "outputs": [],
      "source": [
        "te3_results_df = pd.DataFrame(te3_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MscVRdNCylJ-",
        "outputId": "275beff8-3c59-4063-8270-c01736b4ee05"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(1.0)"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Uses OpenAI’s text-embedding-3-small to embed documents.\n",
        "#Evaluates retrieval accuracy.\n",
        "#Computes hit rate (percentage of queries where the correct document appears in the top results).\n",
        "te3_hit_rate = te3_results_df[\"is_hit\"].mean()\n",
        "te3_hit_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ra-mh0L96dQ"
      },
      "source": [
        "### `Snowflake/snowflake-arctic-embed-l` (base)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "!uv pip install -qU langchain_huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEskxwvFypHe",
        "outputId": "a3aad8ce-48ef-4d8f-9ed0-122b1ec9a000"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'e766257a-5620-4650-8510-36bd822f7741': 'What are some potential societal impacts of the technology mentioned in the context?  ', '9aa330aa-32d7-49c3-b43e-0ab7d73ea8ac': 'Why does the author believe that the knowledge gap between tech enthusiasts and the general population is unhealthy?', '3a2a223d-5f76-42ed-9156-6a528f5128c0': 'What are some reasons people dislike LLMs according to the context?', 'a86fd938-aa11-4500-b0ca-5e699075feb5': 'Why is it important to discuss the criticisms of LLMs?', '1001c601-96e3-488b-bc49-838666f7c584': 'What are some potential consequences of making decisions based on hype and misinformation?  ', 'a449a292-7d6f-4390-bbb3-b0709d4a4f91': 'Why is it important to acknowledge good applications of certain tools before making decisions about their use?', '974ba56d-26a0-447e-8cc3-1fa236345123': \"What is the author's perspective on the environmental impact of plagiarism machines in the field discussed?  \", '94aa9911-47b2-40ce-8e63-0e8483e436a1': 'What does the author believe is necessary to help others understand the value in this field?', '9a4d9e05-c78f-4de8-90e3-f24212fb9ba9': 'What significant event regarding the term \"Artificial Intelligence\" occurred on January 7th?  ', 'a85576f3-e439-4348-ad94-b957ae336930': 'What was discussed on March 5th regarding prompt injection and jailbreaking?', '9b5376e5-900c-49ce-a345-939358334cbb': 'What is the focus of the event scheduled for June 21st regarding AI applications?  ', 'f1987496-c82e-4f58-b234-909ed184df10': 'What does the term \"Slop\" refer to in the context of AI-generated content as mentioned on May 8th?', '5cfdf963-60eb-4ec0-bc4a-9c2af9e18eb9': \"What new feature was added to Claude's API on August 23rd?\", '18b87be5-d098-4244-8994-103b8ac364e4': 'What was discussed in the weeknotes on September 12th regarding OpenAI?', '76a3efdf-30f6-4566-ad0d-31fc1ef186d3': 'What are the key features of NotebookLM’s automatically generated podcasts that make them effective?  ', 'ad5e584d-399c-4f92-8611-d64914d259c5': 'What topics were covered during OpenAI DevDay 2024?', '78766645-cd92-460d-8c3f-37d5f9d35965': 'What is the purpose of the new Claude analysis JavaScript code execution tool mentioned on the 24th?  ', '56b33db5-7cce-4b58-8149-a634113e227c': 'How can users interact with images, audio, and video in their terminal as noted on the 29th?', 'f99f2d2e-a8bb-4fe0-984f-521ef1419235': 'What advancements were made in running GPT-4 class models on personal laptops?', 'f3e6ee77-113a-4a5e-a5c1-4d8f39b502d0': \"How does ChatGPT Canvas's ability to make API requests compare to previous functionalities?\", '3ef0823d-fb66-465c-a161-32e75aa0e4da': 'What is the title of the article posted by Simon Willison on 31st December 2024?', '63edfddb-2d2f-4eea-b225-1a3e91a62454': 'When was the article \"Stuff we figured out about AI in 2023\" published?', '47260ad3-121f-4603-9aae-a3f6ef7ea6a8': 'What is the significance of the number 940 in the context of generative AI?  ', 'b4ef8d52-e66a-4a43-b259-08adb3e878b5': 'How does the term \"long-context\" relate to the other topics mentioned in the context?'}\n"
          ]
        }
      ],
      "source": [
        "#Uses Hugging Face’s Snowflake/snowflake-arctic-embed-l embeddings.\n",
        "#Computes retrieval performance and compares it to OpenAI’s model.\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "huggingface_embeddings = HuggingFaceEmbeddings(model_name=\"Snowflake/snowflake-arctic-embed-l\")\n",
        "arctic_embed_m_results = evaluate_openai(test_dataset, huggingface_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "KlKgiXTWzMTg"
      },
      "outputs": [],
      "source": [
        "arctic_embed_m_results_df = pd.DataFrame(arctic_embed_m_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zV5vJWrJzOhc",
        "outputId": "30049be6-deb9-4ceb-dc13-24d7e125f131"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(0.9166666666666666)"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "arctic_embed_m_hit_rate = arctic_embed_m_results_df[\"is_hit\"].mean()\n",
        "arctic_embed_m_hit_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcR3-0s19_lu"
      },
      "source": [
        "### `Snowflake/snowflake-arctic-embed-l` (fine-tuned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ilse1LduzP1i",
        "outputId": "292ab58f-7594-45fc-a5b8-1062cc553f66"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "46443209e23648198fc5bae0b466cb7c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "12c69cf2094641c59f751e7fcba8d005",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/275 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa71a5b56d0948c69560021718671efd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/29.1k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c1a53eb6713423cb988c92501f16d5a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df0b1ac5c7fc4c7fa3712a62eca74b98",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/641 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a0ad8f894fbe47d0af29f673ea4480cc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertModel were not initialized from the model checkpoint at kcheng0816/finetuned_arctic_kc and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f31b8baf3e5447b99bfbae7fa6e66377",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.41k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "56ebadf7f21e49b9b35d1537cec1b0c1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2a707a52315b4531aa5970eecd456768",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2fed2a4e55cf40f98aeb54ceb2d5eaa8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a6b593b9203e43f5b4aefe10ae91fc4a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "1_Pooling%2Fconfig.json:   0%|          | 0.00/297 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'e766257a-5620-4650-8510-36bd822f7741': 'What are some potential societal impacts of the technology mentioned in the context?  ', '9aa330aa-32d7-49c3-b43e-0ab7d73ea8ac': 'Why does the author believe that the knowledge gap between tech enthusiasts and the general population is unhealthy?', '3a2a223d-5f76-42ed-9156-6a528f5128c0': 'What are some reasons people dislike LLMs according to the context?', 'a86fd938-aa11-4500-b0ca-5e699075feb5': 'Why is it important to discuss the criticisms of LLMs?', '1001c601-96e3-488b-bc49-838666f7c584': 'What are some potential consequences of making decisions based on hype and misinformation?  ', 'a449a292-7d6f-4390-bbb3-b0709d4a4f91': 'Why is it important to acknowledge good applications of certain tools before making decisions about their use?', '974ba56d-26a0-447e-8cc3-1fa236345123': \"What is the author's perspective on the environmental impact of plagiarism machines in the field discussed?  \", '94aa9911-47b2-40ce-8e63-0e8483e436a1': 'What does the author believe is necessary to help others understand the value in this field?', '9a4d9e05-c78f-4de8-90e3-f24212fb9ba9': 'What significant event regarding the term \"Artificial Intelligence\" occurred on January 7th?  ', 'a85576f3-e439-4348-ad94-b957ae336930': 'What was discussed on March 5th regarding prompt injection and jailbreaking?', '9b5376e5-900c-49ce-a345-939358334cbb': 'What is the focus of the event scheduled for June 21st regarding AI applications?  ', 'f1987496-c82e-4f58-b234-909ed184df10': 'What does the term \"Slop\" refer to in the context of AI-generated content as mentioned on May 8th?', '5cfdf963-60eb-4ec0-bc4a-9c2af9e18eb9': \"What new feature was added to Claude's API on August 23rd?\", '18b87be5-d098-4244-8994-103b8ac364e4': 'What was discussed in the weeknotes on September 12th regarding OpenAI?', '76a3efdf-30f6-4566-ad0d-31fc1ef186d3': 'What are the key features of NotebookLM’s automatically generated podcasts that make them effective?  ', 'ad5e584d-399c-4f92-8611-d64914d259c5': 'What topics were covered during OpenAI DevDay 2024?', '78766645-cd92-460d-8c3f-37d5f9d35965': 'What is the purpose of the new Claude analysis JavaScript code execution tool mentioned on the 24th?  ', '56b33db5-7cce-4b58-8149-a634113e227c': 'How can users interact with images, audio, and video in their terminal as noted on the 29th?', 'f99f2d2e-a8bb-4fe0-984f-521ef1419235': 'What advancements were made in running GPT-4 class models on personal laptops?', 'f3e6ee77-113a-4a5e-a5c1-4d8f39b502d0': \"How does ChatGPT Canvas's ability to make API requests compare to previous functionalities?\", '3ef0823d-fb66-465c-a161-32e75aa0e4da': 'What is the title of the article posted by Simon Willison on 31st December 2024?', '63edfddb-2d2f-4eea-b225-1a3e91a62454': 'When was the article \"Stuff we figured out about AI in 2023\" published?', '47260ad3-121f-4603-9aae-a3f6ef7ea6a8': 'What is the significance of the number 940 in the context of generative AI?  ', 'b4ef8d52-e66a-4a43-b259-08adb3e878b5': 'How does the term \"long-context\" relate to the other topics mentioned in the context?'}\n"
          ]
        }
      ],
      "source": [
        "#Evaluates the fine-tuned Arctic embeddings.\n",
        "#Measures how well the fine-tuned model improves retrieval accuracy.\n",
        "finetune_embeddings = HuggingFaceEmbeddings(model_name=\"kcheng0816/finetuned_arctic_kc\")\n",
        "finetune_results = evaluate_openai(test_dataset, finetune_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "xxhZPqkNzZlh"
      },
      "outputs": [],
      "source": [
        "finetune_results_df = pd.DataFrame(finetune_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4thAK2BXzaj6",
        "outputId": "e890b5d1-86b7-4bfe-8ffe-a779a132e0c6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(1.0)"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "finetune_hit_rate = finetune_results_df[\"is_hit\"].mean()\n",
        "finetune_hit_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iegFM209mBk3"
      },
      "source": [
        "## Task 1: Vibe Checking the RAG Pipeline\n",
        "\n",
        "We're going to use our RAG pipeline to vibe check on some common phrases now that we've modified it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xzg0AA5krgR4"
      },
      "source": [
        "### Creating New Chunks\n",
        "\n",
        "In order to try and evaluate our system more fairly, let's create new chunks that we will use to create our Vector Store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "KwQ2_LqNr0Tw"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 600,\n",
        "    chunk_overlap  = 50,\n",
        "    length_function = len\n",
        ")\n",
        "\n",
        "training_documents = text_splitter.split_documents(text_loader.load())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIdxahHXpP-c"
      },
      "source": [
        "### Base Chain\n",
        "\n",
        "We'll start by constructing our base chain, which will use the untrained retrieval model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOsxIXpNpWC2"
      },
      "source": [
        "#### R - Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "azIGIKYfmNCT"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "base_vectorstore = FAISS.from_documents(training_documents, huggingface_embeddings)\n",
        "base_retriever = base_vectorstore.as_retriever(search_kwargs={\"k\": 6})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-1nVZ0KpX5N"
      },
      "source": [
        "#### A - Augmented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "G10Fr-aKojeA"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_PROMPT = \"\"\"\\\n",
        "Given a provided context and a question, you must answer the question. If you do not know the answer, you must state that you do not know.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt_template = ChatPromptTemplate.from_template(RAG_PROMPT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Euq6RQEopZvD"
      },
      "source": [
        "#### G - Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "5-mfbbrypMHG"
      },
      "outputs": [],
      "source": [
        "rag_llm =  ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ2p4mnUpbYY"
      },
      "source": [
        "#### RAG - LCEL RAG Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "ssuR-LaboyGq"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "\n",
        "base_rag_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | base_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt_template | rag_llm | StrOutputParser(), \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "emm6WbB9pfKt",
        "outputId": "f0e0c83f-c617-493e-99ce-869061a315f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'An agent, in the context of AI, is an infuriatingly vague term that generally refers to AI systems that can act on your behalf. There are two main interpretations: one sees agents as systems that go and perform tasks for you (like a travel agent), while the other views them as LLMs (large language models) that have access to tools and can run processes in a loop to solve problems. However, the term lacks a single, clear definition, leading to confusion about its meaning and utility.'"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_rag_chain.invoke({\"question\" : \"What is an agent?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "mUOrd0OBprAq",
        "outputId": "0070d677-0bde-48be-a8a3-f0c53b9eee90"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Organizations that have produced better-than-GPT-3 class models include Anthropic, Mistral, Google, Meta, EleutherAI, Stability AI, TII in Abu Dhabi (Falcon), Microsoft Research, xAI, Replit, Baidu, and several others.'"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_rag_chain.invoke({\"question\" : \"Who has produced better models than GPT-3?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "OnfuFl59py7I",
        "outputId": "37480e29-17a6-40e2-fe8a-7eb6ea270569"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'I do not know.'"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_rag_chain.invoke({\"question\" : \"What is the laziest AI month?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "-NmqwHBDqTZ8",
        "outputId": "86d7b59a-97c6-4b65-805f-ae449e3b1a20"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'I do not know.'"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_rag_chain.invoke({\"question\" : \"What is the largest model that Simon has run on his phone?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqNS0UJAp3lC"
      },
      "source": [
        "### Fine-tuned Embedding Model\n",
        "\n",
        "Now let's rebuild our RAG chain with the Fine-tuned model - the only component we need to change is our `FAISS` vectorstore!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "ihO7tP6mqATy"
      },
      "outputs": [],
      "source": [
        "finetune_vectorstore = FAISS.from_documents(training_documents, finetune_embeddings)\n",
        "finetune_retriever = finetune_vectorstore.as_retriever(search_kwargs={\"k\": 6})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "1_cIFvWzqKGY"
      },
      "outputs": [],
      "source": [
        "finetune_rag_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | finetune_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt_template | rag_llm | StrOutputParser(), \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "OJmRHJF2qNgj",
        "outputId": "b99d4b58-8487-48ec-ee6c-8ea93f9b0192"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'An \"agent\" in the context of AI refers to a system that can act on behalf of a user, but the term lacks a single, clear definition. There are two main interpretations: one sees agents as entities that perform tasks for users, similar to a travel agent, while the other views them as LLMs (Large Language Models) that utilize tools to solve problems in a loop. However, the concept remains vague and is often associated with challenges such as gullibility, where these systems may struggle to distinguish truth from fiction. Overall, the term \"agent\" is still evolving and has not yet been fully realized in practical applications.'"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "finetune_rag_chain.invoke({\"question\" : \"What is an Agent?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "EnK-c2ugqPPh",
        "outputId": "b8300e0a-1b51-48dd-93c3-254e0aa84e36"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Organizations that have produced better models than GPT-3 include Anthropic, Mistral, Google, Meta, EleutherAI, Stability AI, TII in Abu Dhabi (Falcon), Microsoft Research, xAI, Replit, Baidu, and several others.'"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "finetune_rag_chain.invoke({\"question\" : \"Who has produced better models than GPT-3?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "83hssg1AWozc",
        "outputId": "8d1ef134-7889-4379-a49b-6c0a476b7a1a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'I do not know.'"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "finetune_rag_chain.invoke({\"question\" : \"What is the laziest AI month?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "rsHmGeFbqRET",
        "outputId": "1ab223c2-d2fc-46ac-8541-af038de3166d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The largest model that Simon has run on his phone is the Llama 3.2 3B model.'"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "finetune_rag_chain.invoke({\"question\" : \"What is the largest model that Simon has run on his phone?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDgD8seY_I3W"
      },
      "source": [
        "####❓Question #2:\n",
        "\n",
        "Which LCEL RAG Chain do you think answered the questions better, and why?\n",
        "\n",
        "**ANSWER**\n",
        "\n",
        "finetune_rag_chain answered the 1st and 2nd questions same good as base_rag_chain and also it did answer the 4th query related to Simon correctly. Because the embedding model kcheng0816/finetuned_arctic_kc used in the finetune_retriever was fine tuned by the specific information in Simon's web pages, the basic embedding model used in base_retriever does not have the information from Simon's web pages. \n",
        "\n",
        "Both chains did not answer the 3rd question because the provided contexts did not provide the related information. This result matched the rag prompt template."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCbq1sZArIx4"
      },
      "source": [
        "## Task 2: RAGAS Evaluation\n",
        "\n",
        "It's great to have some idea of how our system is doing based on vibe-checks, but let's use RAGAS to provide more insight info. on how things are improving!\n",
        "\n",
        "> NOTE: Please recreate *exactly* the RAGAS process we used to evaluate RAG, baselining with the default retriever, and then comparing the new retriever. The includes the Synthetic Data Generation steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "jq880DtHk9pX"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "!uv pip install -qU ragas==0.2.12 langchain-community==0.3.14 langchain-openai==0.2.14 unstructured==0.16.12 langgraph==0.2.61 langchain-qdrant==0.2.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Input RAGAS API key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"RAGAS_APP_TOKEN\"] = getpass(\"Please enter your Ragas API key!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load downloaded web pages from data folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
            "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "\n",
        "path = \"data/\"\n",
        "loader = DirectoryLoader(path, glob=\"*.html\")\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create your embeding model wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
        "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate Synthetic Data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "46b887219595432d96b36228d3d88204",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying HeadlinesExtractor:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9c40232e39b64499b61a081da3991f9d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying HeadlineSplitter:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c3ea7644167042c48869ab13d114cd05",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying SummaryExtractor:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e8db206ec8104ec9b34e354c8b715e73",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying CustomNodeFilter:   0%|          | 0/12 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4ea7325a04ca41ba85b40f86fdec5cf2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/26 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5765b659658b456ca956351659c92be3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "63b4b8392a134f8483c490af4ddce176",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating personas:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ccafaf0953b34158bd17fac04dc90241",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Scenarios:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0fa75fa4341c44898bbb2dea688625a5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Samples:   0%|          | 0/12 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from ragas.testset import TestsetGenerator\n",
        "\n",
        "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
        "dataset = generator.generate_with_langchain_docs(docs, testset_size=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>reference</th>\n",
              "      <th>synthesizer_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What role has Microsoft Research played in the...</td>\n",
              "      <td>[Code may be the best application The ethics o...</td>\n",
              "      <td>Microsoft Research is one of the organizations...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What are some challenges associated with evalu...</td>\n",
              "      <td>[Based Development As a computer scientist and...</td>\n",
              "      <td>Evaluating LLMs is challenging because there a...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Wht were the key developments in AI during 2023?</td>\n",
              "      <td>[Simon Willison’s Weblog Subscribe Stuff we fi...</td>\n",
              "      <td>2023 was the breakthrough year for Large Langu...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What is the significance of OpenAI in the cont...</td>\n",
              "      <td>[easy to follow. The rest of the document incl...</td>\n",
              "      <td>OpenAI is a significant entity in the context ...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How do the ethics of AI training data and the ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nCode may be the best application T...</td>\n",
              "      <td>The intersection of the ethics of AI training ...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>How do the ethics of AI training data and the ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nCode may be the best application T...</td>\n",
              "      <td>The ethics of AI training data and the environ...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How have advancements in Large Language Models...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nCode may be the best application T...</td>\n",
              "      <td>Advancements in Large Language Models (LLMs) h...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>How have advancements in Large Language Models...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nCode may be the best application T...</td>\n",
              "      <td>Advancements in Large Language Models (LLMs) h...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>How have advancements in GPT-4o and its pricin...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nSimon Willison’s Weblog Subscribe ...</td>\n",
              "      <td>Advancements in GPT-4o have significantly impa...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>What are some of the major advancements and ch...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nSimon Willison’s Weblog Subscribe ...</td>\n",
              "      <td>In 2024, significant advancements in Large Lan...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>How did the advancements in Large Language Mod...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nSimon Willison’s Weblog Subscribe ...</td>\n",
              "      <td>In 2023, Large Language Models (LLMs) marked a...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>What are the challenges and advancements in LL...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nBased Development As a computer sc...</td>\n",
              "      <td>In Simon Willison's 2024 review, several chall...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           user_input  \\\n",
              "0   What role has Microsoft Research played in the...   \n",
              "1   What are some challenges associated with evalu...   \n",
              "2    Wht were the key developments in AI during 2023?   \n",
              "3   What is the significance of OpenAI in the cont...   \n",
              "4   How do the ethics of AI training data and the ...   \n",
              "5   How do the ethics of AI training data and the ...   \n",
              "6   How have advancements in Large Language Models...   \n",
              "7   How have advancements in Large Language Models...   \n",
              "8   How have advancements in GPT-4o and its pricin...   \n",
              "9   What are some of the major advancements and ch...   \n",
              "10  How did the advancements in Large Language Mod...   \n",
              "11  What are the challenges and advancements in LL...   \n",
              "\n",
              "                                   reference_contexts  \\\n",
              "0   [Code may be the best application The ethics o...   \n",
              "1   [Based Development As a computer scientist and...   \n",
              "2   [Simon Willison’s Weblog Subscribe Stuff we fi...   \n",
              "3   [easy to follow. The rest of the document incl...   \n",
              "4   [<1-hop>\\n\\nCode may be the best application T...   \n",
              "5   [<1-hop>\\n\\nCode may be the best application T...   \n",
              "6   [<1-hop>\\n\\nCode may be the best application T...   \n",
              "7   [<1-hop>\\n\\nCode may be the best application T...   \n",
              "8   [<1-hop>\\n\\nSimon Willison’s Weblog Subscribe ...   \n",
              "9   [<1-hop>\\n\\nSimon Willison’s Weblog Subscribe ...   \n",
              "10  [<1-hop>\\n\\nSimon Willison’s Weblog Subscribe ...   \n",
              "11  [<1-hop>\\n\\nBased Development As a computer sc...   \n",
              "\n",
              "                                            reference  \\\n",
              "0   Microsoft Research is one of the organizations...   \n",
              "1   Evaluating LLMs is challenging because there a...   \n",
              "2   2023 was the breakthrough year for Large Langu...   \n",
              "3   OpenAI is a significant entity in the context ...   \n",
              "4   The intersection of the ethics of AI training ...   \n",
              "5   The ethics of AI training data and the environ...   \n",
              "6   Advancements in Large Language Models (LLMs) h...   \n",
              "7   Advancements in Large Language Models (LLMs) h...   \n",
              "8   Advancements in GPT-4o have significantly impa...   \n",
              "9   In 2024, significant advancements in Large Lan...   \n",
              "10  In 2023, Large Language Models (LLMs) marked a...   \n",
              "11  In Simon Willison's 2024 review, several chall...   \n",
              "\n",
              "                        synthesizer_name  \n",
              "0   single_hop_specifc_query_synthesizer  \n",
              "1   single_hop_specifc_query_synthesizer  \n",
              "2   single_hop_specifc_query_synthesizer  \n",
              "3   single_hop_specifc_query_synthesizer  \n",
              "4   multi_hop_abstract_query_synthesizer  \n",
              "5   multi_hop_abstract_query_synthesizer  \n",
              "6   multi_hop_abstract_query_synthesizer  \n",
              "7   multi_hop_abstract_query_synthesizer  \n",
              "8   multi_hop_specific_query_synthesizer  \n",
              "9   multi_hop_specific_query_synthesizer  \n",
              "10  multi_hop_specific_query_synthesizer  \n",
              "11  multi_hop_specific_query_synthesizer  "
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testset uploaded! View at https://app.ragas.io/dashboard/alignment/testset/145473f5-76da-40fd-83a2-53496fe809c5\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'https://app.ragas.io/dashboard/alignment/testset/145473f5-76da-40fd-83a2-53496fe809c5'"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Call the base RAG chain with synthetic data and save the repsonses and the retrieved contexts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [],
      "source": [
        "for test_row in dataset:\n",
        "  response = base_rag_chain.invoke({\"question\" : test_row.eval_sample.user_input})\n",
        "  test_row.eval_sample.response = response[\"response\"]\n",
        "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>retrieved_contexts</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>response</th>\n",
              "      <th>reference</th>\n",
              "      <th>synthesizer_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What role has Microsoft Research played in the...</td>\n",
              "      <td>[Prompt injection explained, with video, slide...</td>\n",
              "      <td>[Code may be the best application The ethics o...</td>\n",
              "      <td>I do not know.</td>\n",
              "      <td>Microsoft Research is one of the organizations...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What are some challenges associated with evalu...</td>\n",
              "      <td>[I get it. There are plenty of reasons to disl...</td>\n",
              "      <td>[Based Development As a computer scientist and...</td>\n",
              "      <td>Some challenges associated with evaluating LLM...</td>\n",
              "      <td>Evaluating LLMs is challenging because there a...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Wht were the key developments in AI during 2023?</td>\n",
              "      <td>[The legal arguments here are complex. I’m not...</td>\n",
              "      <td>[Simon Willison’s Weblog Subscribe Stuff we fi...</td>\n",
              "      <td>The key developments in AI during 2023 include...</td>\n",
              "      <td>2023 was the breakthrough year for Large Langu...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What is the significance of OpenAI in the cont...</td>\n",
              "      <td>[The legal arguments here are complex. I’m not...</td>\n",
              "      <td>[easy to follow. The rest of the document incl...</td>\n",
              "      <td>The provided context does not specifically men...</td>\n",
              "      <td>OpenAI is a significant entity in the context ...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How do the ethics of AI training data and the ...</td>\n",
              "      <td>[The legal arguments here are complex. I’m not...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nCode may be the best application T...</td>\n",
              "      <td>The ethics of AI training data and the ease of...</td>\n",
              "      <td>The intersection of the ethics of AI training ...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>How do the ethics of AI training data and the ...</td>\n",
              "      <td>[The legal arguments here are complex. I’m not...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nCode may be the best application T...</td>\n",
              "      <td>The ethics of AI training data and the environ...</td>\n",
              "      <td>The ethics of AI training data and the environ...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How have advancements in Large Language Models...</td>\n",
              "      <td>[I get it. There are plenty of reasons to disl...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nCode may be the best application T...</td>\n",
              "      <td>Advancements in Large Language Models (LLMs) h...</td>\n",
              "      <td>Advancements in Large Language Models (LLMs) h...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>How have advancements in Large Language Models...</td>\n",
              "      <td>[I get it. There are plenty of reasons to disl...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nCode may be the best application T...</td>\n",
              "      <td>The provided context does not contain specific...</td>\n",
              "      <td>Advancements in Large Language Models (LLMs) h...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>How have advancements in GPT-4o and its pricin...</td>\n",
              "      <td>[That same laptop that could just about run a ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nSimon Willison’s Weblog Subscribe ...</td>\n",
              "      <td>Advancements in GPT-4 and its pricing have sig...</td>\n",
              "      <td>Advancements in GPT-4o have significantly impa...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>What are some of the major advancements and ch...</td>\n",
              "      <td>[Even the openly licensed ones are still the w...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nSimon Willison’s Weblog Subscribe ...</td>\n",
              "      <td>In 2024, some major advancements in Large Lang...</td>\n",
              "      <td>In 2024, significant advancements in Large Lan...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>How did the advancements in Large Language Mod...</td>\n",
              "      <td>[The legal arguments here are complex. I’m not...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nSimon Willison’s Weblog Subscribe ...</td>\n",
              "      <td>The advancements in Large Language Models (LLM...</td>\n",
              "      <td>In 2023, Large Language Models (LLMs) marked a...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>What are the challenges and advancements in LL...</td>\n",
              "      <td>[Even the openly licensed ones are still the w...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nBased Development As a computer sc...</td>\n",
              "      <td>In Simon Willison's 2024 review, the challenge...</td>\n",
              "      <td>In Simon Willison's 2024 review, several chall...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           user_input  \\\n",
              "0   What role has Microsoft Research played in the...   \n",
              "1   What are some challenges associated with evalu...   \n",
              "2    Wht were the key developments in AI during 2023?   \n",
              "3   What is the significance of OpenAI in the cont...   \n",
              "4   How do the ethics of AI training data and the ...   \n",
              "5   How do the ethics of AI training data and the ...   \n",
              "6   How have advancements in Large Language Models...   \n",
              "7   How have advancements in Large Language Models...   \n",
              "8   How have advancements in GPT-4o and its pricin...   \n",
              "9   What are some of the major advancements and ch...   \n",
              "10  How did the advancements in Large Language Mod...   \n",
              "11  What are the challenges and advancements in LL...   \n",
              "\n",
              "                                   retrieved_contexts  \\\n",
              "0   [Prompt injection explained, with video, slide...   \n",
              "1   [I get it. There are plenty of reasons to disl...   \n",
              "2   [The legal arguments here are complex. I’m not...   \n",
              "3   [The legal arguments here are complex. I’m not...   \n",
              "4   [The legal arguments here are complex. I’m not...   \n",
              "5   [The legal arguments here are complex. I’m not...   \n",
              "6   [I get it. There are plenty of reasons to disl...   \n",
              "7   [I get it. There are plenty of reasons to disl...   \n",
              "8   [That same laptop that could just about run a ...   \n",
              "9   [Even the openly licensed ones are still the w...   \n",
              "10  [The legal arguments here are complex. I’m not...   \n",
              "11  [Even the openly licensed ones are still the w...   \n",
              "\n",
              "                                   reference_contexts  \\\n",
              "0   [Code may be the best application The ethics o...   \n",
              "1   [Based Development As a computer scientist and...   \n",
              "2   [Simon Willison’s Weblog Subscribe Stuff we fi...   \n",
              "3   [easy to follow. The rest of the document incl...   \n",
              "4   [<1-hop>\\n\\nCode may be the best application T...   \n",
              "5   [<1-hop>\\n\\nCode may be the best application T...   \n",
              "6   [<1-hop>\\n\\nCode may be the best application T...   \n",
              "7   [<1-hop>\\n\\nCode may be the best application T...   \n",
              "8   [<1-hop>\\n\\nSimon Willison’s Weblog Subscribe ...   \n",
              "9   [<1-hop>\\n\\nSimon Willison’s Weblog Subscribe ...   \n",
              "10  [<1-hop>\\n\\nSimon Willison’s Weblog Subscribe ...   \n",
              "11  [<1-hop>\\n\\nBased Development As a computer sc...   \n",
              "\n",
              "                                             response  \\\n",
              "0                                      I do not know.   \n",
              "1   Some challenges associated with evaluating LLM...   \n",
              "2   The key developments in AI during 2023 include...   \n",
              "3   The provided context does not specifically men...   \n",
              "4   The ethics of AI training data and the ease of...   \n",
              "5   The ethics of AI training data and the environ...   \n",
              "6   Advancements in Large Language Models (LLMs) h...   \n",
              "7   The provided context does not contain specific...   \n",
              "8   Advancements in GPT-4 and its pricing have sig...   \n",
              "9   In 2024, some major advancements in Large Lang...   \n",
              "10  The advancements in Large Language Models (LLM...   \n",
              "11  In Simon Willison's 2024 review, the challenge...   \n",
              "\n",
              "                                            reference  \\\n",
              "0   Microsoft Research is one of the organizations...   \n",
              "1   Evaluating LLMs is challenging because there a...   \n",
              "2   2023 was the breakthrough year for Large Langu...   \n",
              "3   OpenAI is a significant entity in the context ...   \n",
              "4   The intersection of the ethics of AI training ...   \n",
              "5   The ethics of AI training data and the environ...   \n",
              "6   Advancements in Large Language Models (LLMs) h...   \n",
              "7   Advancements in Large Language Models (LLMs) h...   \n",
              "8   Advancements in GPT-4o have significantly impa...   \n",
              "9   In 2024, significant advancements in Large Lan...   \n",
              "10  In 2023, Large Language Models (LLMs) marked a...   \n",
              "11  In Simon Willison's 2024 review, several chall...   \n",
              "\n",
              "                        synthesizer_name  \n",
              "0   single_hop_specifc_query_synthesizer  \n",
              "1   single_hop_specifc_query_synthesizer  \n",
              "2   single_hop_specifc_query_synthesizer  \n",
              "3   single_hop_specifc_query_synthesizer  \n",
              "4   multi_hop_abstract_query_synthesizer  \n",
              "5   multi_hop_abstract_query_synthesizer  \n",
              "6   multi_hop_abstract_query_synthesizer  \n",
              "7   multi_hop_abstract_query_synthesizer  \n",
              "8   multi_hop_specific_query_synthesizer  \n",
              "9   multi_hop_specific_query_synthesizer  \n",
              "10  multi_hop_specific_query_synthesizer  \n",
              "11  multi_hop_specific_query_synthesizer  "
            ]
          },
          "execution_count": 108,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Convert to Evaluation Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas import EvaluationDataset\n",
        "\n",
        "evaluation_dataset = EvaluationDataset.from_pandas(dataset.to_pandas())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Select a judge model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas import evaluate\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate the base RAG chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "33294f84620a4c559c8f600e438c48d9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/72 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception raised in Job[2]: TypeError(ufunc 'invert' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe'')\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'context_recall': 0.3995, 'faithfulness': 0.6499, 'factual_correctness': 0.3300, 'answer_relevancy': 0.7198, 'context_entity_recall': 0.2865, 'noise_sensitivity_relevant': 0.2296}"
            ]
          },
          "execution_count": 111,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, ResponseRelevancy, ContextEntityRecall, NoiseSensitivity\n",
        "from ragas import evaluate, RunConfig\n",
        "\n",
        "custom_run_config = RunConfig(timeout=1440)\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=evaluation_dataset,\n",
        "    metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness(), ResponseRelevancy(), ContextEntityRecall(), NoiseSensitivity()],\n",
        "    llm=evaluator_llm,\n",
        "    run_config=custom_run_config\n",
        ")\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Call the base RAG chain with synthetic data and save the repsonses and the retrieved contexts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [],
      "source": [
        "for test_row in dataset:\n",
        "  response = finetune_rag_chain.invoke({\"question\" : test_row.eval_sample.user_input})\n",
        "  test_row.eval_sample.response = response[\"response\"]\n",
        "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>retrieved_contexts</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>response</th>\n",
              "      <th>reference</th>\n",
              "      <th>synthesizer_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What role has Microsoft Research played in the...</td>\n",
              "      <td>[Large Language Models\\nThey’re actually quite...</td>\n",
              "      <td>[Code may be the best application The ethics o...</td>\n",
              "      <td>Microsoft Research has contributed to the deve...</td>\n",
              "      <td>Microsoft Research is one of the organizations...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What are some challenges associated with evalu...</td>\n",
              "      <td>[I find I have to work with an LLM for a few w...</td>\n",
              "      <td>[Based Development As a computer scientist and...</td>\n",
              "      <td>Some challenges associated with evaluating LLM...</td>\n",
              "      <td>Evaluating LLMs is challenging because there a...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Wht were the key developments in AI during 2023?</td>\n",
              "      <td>[Stuff we figured out about AI in 2023\\n\\n\\n\\n...</td>\n",
              "      <td>[Simon Willison’s Weblog Subscribe Stuff we fi...</td>\n",
              "      <td>The key developments in AI during 2023 include...</td>\n",
              "      <td>2023 was the breakthrough year for Large Langu...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What is the significance of OpenAI in the cont...</td>\n",
              "      <td>[Here’s the sequel to this post: Things we lea...</td>\n",
              "      <td>[easy to follow. The rest of the document incl...</td>\n",
              "      <td>OpenAI is significant in the context of large ...</td>\n",
              "      <td>OpenAI is a significant entity in the context ...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How do the ethics of AI training data and the ...</td>\n",
              "      <td>[The legal arguments here are complex. I’m not...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nCode may be the best application T...</td>\n",
              "      <td>The ethics of AI training data and the ease of...</td>\n",
              "      <td>The intersection of the ethics of AI training ...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>How do the ethics of AI training data and the ...</td>\n",
              "      <td>[The legal arguments here are complex. I’m not...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nCode may be the best application T...</td>\n",
              "      <td>The ethics of AI training data and the environ...</td>\n",
              "      <td>The ethics of AI training data and the environ...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How have advancements in Large Language Models...</td>\n",
              "      <td>[The rise of inference-scaling “reasoning” mod...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nCode may be the best application T...</td>\n",
              "      <td>Advancements in Large Language Models (LLMs) h...</td>\n",
              "      <td>Advancements in Large Language Models (LLMs) h...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>How have advancements in Large Language Models...</td>\n",
              "      <td>[The rise of inference-scaling “reasoning” mod...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nCode may be the best application T...</td>\n",
              "      <td>Advancements in Large Language Models (LLMs) h...</td>\n",
              "      <td>Advancements in Large Language Models (LLMs) h...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>How have advancements in GPT-4o and its pricin...</td>\n",
              "      <td>[The GPT-4 barrier was comprehensively broken\\...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nSimon Willison’s Weblog Subscribe ...</td>\n",
              "      <td>Advancements in GPT-4o have significantly impa...</td>\n",
              "      <td>Advancements in GPT-4o have significantly impa...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>What are some of the major advancements and ch...</td>\n",
              "      <td>[The rise of inference-scaling “reasoning” mod...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nSimon Willison’s Weblog Subscribe ...</td>\n",
              "      <td>In 2024, some major advancements in large lang...</td>\n",
              "      <td>In 2024, significant advancements in Large Lan...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>How did the advancements in Large Language Mod...</td>\n",
              "      <td>[Stuff we figured out about AI in 2023\\n\\n\\n\\n...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nSimon Willison’s Weblog Subscribe ...</td>\n",
              "      <td>The advancements in Large Language Models (LLM...</td>\n",
              "      <td>In 2023, Large Language Models (LLMs) marked a...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>What are the challenges and advancements in LL...</td>\n",
              "      <td>[More recent articles\\n\\nURL-addressable Pyodi...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nBased Development As a computer sc...</td>\n",
              "      <td>In Simon Willison's 2024 review of Large Langu...</td>\n",
              "      <td>In Simon Willison's 2024 review, several chall...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           user_input  \\\n",
              "0   What role has Microsoft Research played in the...   \n",
              "1   What are some challenges associated with evalu...   \n",
              "2    Wht were the key developments in AI during 2023?   \n",
              "3   What is the significance of OpenAI in the cont...   \n",
              "4   How do the ethics of AI training data and the ...   \n",
              "5   How do the ethics of AI training data and the ...   \n",
              "6   How have advancements in Large Language Models...   \n",
              "7   How have advancements in Large Language Models...   \n",
              "8   How have advancements in GPT-4o and its pricin...   \n",
              "9   What are some of the major advancements and ch...   \n",
              "10  How did the advancements in Large Language Mod...   \n",
              "11  What are the challenges and advancements in LL...   \n",
              "\n",
              "                                   retrieved_contexts  \\\n",
              "0   [Large Language Models\\nThey’re actually quite...   \n",
              "1   [I find I have to work with an LLM for a few w...   \n",
              "2   [Stuff we figured out about AI in 2023\\n\\n\\n\\n...   \n",
              "3   [Here’s the sequel to this post: Things we lea...   \n",
              "4   [The legal arguments here are complex. I’m not...   \n",
              "5   [The legal arguments here are complex. I’m not...   \n",
              "6   [The rise of inference-scaling “reasoning” mod...   \n",
              "7   [The rise of inference-scaling “reasoning” mod...   \n",
              "8   [The GPT-4 barrier was comprehensively broken\\...   \n",
              "9   [The rise of inference-scaling “reasoning” mod...   \n",
              "10  [Stuff we figured out about AI in 2023\\n\\n\\n\\n...   \n",
              "11  [More recent articles\\n\\nURL-addressable Pyodi...   \n",
              "\n",
              "                                   reference_contexts  \\\n",
              "0   [Code may be the best application The ethics o...   \n",
              "1   [Based Development As a computer scientist and...   \n",
              "2   [Simon Willison’s Weblog Subscribe Stuff we fi...   \n",
              "3   [easy to follow. The rest of the document incl...   \n",
              "4   [<1-hop>\\n\\nCode may be the best application T...   \n",
              "5   [<1-hop>\\n\\nCode may be the best application T...   \n",
              "6   [<1-hop>\\n\\nCode may be the best application T...   \n",
              "7   [<1-hop>\\n\\nCode may be the best application T...   \n",
              "8   [<1-hop>\\n\\nSimon Willison’s Weblog Subscribe ...   \n",
              "9   [<1-hop>\\n\\nSimon Willison’s Weblog Subscribe ...   \n",
              "10  [<1-hop>\\n\\nSimon Willison’s Weblog Subscribe ...   \n",
              "11  [<1-hop>\\n\\nBased Development As a computer sc...   \n",
              "\n",
              "                                             response  \\\n",
              "0   Microsoft Research has contributed to the deve...   \n",
              "1   Some challenges associated with evaluating LLM...   \n",
              "2   The key developments in AI during 2023 include...   \n",
              "3   OpenAI is significant in the context of large ...   \n",
              "4   The ethics of AI training data and the ease of...   \n",
              "5   The ethics of AI training data and the environ...   \n",
              "6   Advancements in Large Language Models (LLMs) h...   \n",
              "7   Advancements in Large Language Models (LLMs) h...   \n",
              "8   Advancements in GPT-4o have significantly impa...   \n",
              "9   In 2024, some major advancements in large lang...   \n",
              "10  The advancements in Large Language Models (LLM...   \n",
              "11  In Simon Willison's 2024 review of Large Langu...   \n",
              "\n",
              "                                            reference  \\\n",
              "0   Microsoft Research is one of the organizations...   \n",
              "1   Evaluating LLMs is challenging because there a...   \n",
              "2   2023 was the breakthrough year for Large Langu...   \n",
              "3   OpenAI is a significant entity in the context ...   \n",
              "4   The intersection of the ethics of AI training ...   \n",
              "5   The ethics of AI training data and the environ...   \n",
              "6   Advancements in Large Language Models (LLMs) h...   \n",
              "7   Advancements in Large Language Models (LLMs) h...   \n",
              "8   Advancements in GPT-4o have significantly impa...   \n",
              "9   In 2024, significant advancements in Large Lan...   \n",
              "10  In 2023, Large Language Models (LLMs) marked a...   \n",
              "11  In Simon Willison's 2024 review, several chall...   \n",
              "\n",
              "                        synthesizer_name  \n",
              "0   single_hop_specifc_query_synthesizer  \n",
              "1   single_hop_specifc_query_synthesizer  \n",
              "2   single_hop_specifc_query_synthesizer  \n",
              "3   single_hop_specifc_query_synthesizer  \n",
              "4   multi_hop_abstract_query_synthesizer  \n",
              "5   multi_hop_abstract_query_synthesizer  \n",
              "6   multi_hop_abstract_query_synthesizer  \n",
              "7   multi_hop_abstract_query_synthesizer  \n",
              "8   multi_hop_specific_query_synthesizer  \n",
              "9   multi_hop_specific_query_synthesizer  \n",
              "10  multi_hop_specific_query_synthesizer  \n",
              "11  multi_hop_specific_query_synthesizer  "
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas import EvaluationDataset\n",
        "\n",
        "evaluation_dataset = EvaluationDataset.from_pandas(dataset.to_pandas())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate the fine tune RAG chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3ff678bcacff432e9fd4580016a5af25",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/72 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'context_recall': 0.6041, 'faithfulness': 0.8392, 'factual_correctness': 0.4667, 'answer_relevancy': 0.9604, 'context_entity_recall': 0.3552, 'noise_sensitivity_relevant': 0.3281}"
            ]
          },
          "execution_count": 115,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, ResponseRelevancy, ContextEntityRecall, NoiseSensitivity\n",
        "from ragas import evaluate, RunConfig\n",
        "\n",
        "custom_run_config = RunConfig(timeout=1440)\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=evaluation_dataset,\n",
        "    metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness(), ResponseRelevancy(), ContextEntityRecall(), NoiseSensitivity()],\n",
        "    llm=evaluator_llm,\n",
        "    run_config=custom_run_config\n",
        ")\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llmops-course",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "006bf6f5ebaf400486a6b82610381db0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d3fc6edfdab4fe9aff8e805632eadad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ebe8aa7a82124b57bce2d826c1dea0fa",
              "IPY_MODEL_aa11c10b4234452d9430744ab89b59a2",
              "IPY_MODEL_ca05cbcd72cb41d9856804ffb17b26cb"
            ],
            "layout": "IPY_MODEL_cc2a33e9a7ac4c5699346fcbf53b7c95"
          }
        },
        "1016780729b04ba489d488922173ddae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "CheckboxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_9198dd0fa8f04aa7b75307aa2d513bfb",
            "style": "IPY_MODEL_59cd26ae53024fbd85431b6683cd119c",
            "value": true
          }
        },
        "13e9bf583f6442d395334947379a280a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18825dc83221412ab602830fc00db71b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6ea48a80c194d128959422368aa0e10",
            "placeholder": "​",
            "style": "IPY_MODEL_9f4026c62c60493caa18c014ae414e65",
            "value": "Connecting..."
          }
        },
        "1995b4d98fe044e38f1980d47033ca40": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a872283afaa4a33a9bc3f9e57b3650b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "381c780ce17e4662981175400fb8a0f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "408f4dfce21a45cfad36047677ec8658": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a723c6a56e94309b2e3abe63f28aedc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_869814ecd49e46f9a33dd53130e6953f",
            "max": 1336413848,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cc7d460d3c5a4ac6a9b64929736d6598",
            "value": 1336413848
          }
        },
        "4e2c257232c3472697e03350de43cb30": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e7ccd97042c4fb9a201b6dc76762e04": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58699484312f460cb96ac44e3af14aa2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_d4acc9a7aca04564bfaefe33de079394"
          }
        },
        "59cd26ae53024fbd85431b6683cd119c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "72ec09e2cbbf4788b1561abfcdd0819a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "869814ecd49e46f9a33dd53130e6953f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "878dc96f87dd45f389948a546db33e94": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9198dd0fa8f04aa7b75307aa2d513bfb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95160a05de5b402b9bdb0bd2d099cd00": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9638a0456e0c41b1b9b9757932f55c53": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d34dac405fd40139d5dc04eecef55ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_4e7ccd97042c4fb9a201b6dc76762e04",
            "style": "IPY_MODEL_72ec09e2cbbf4788b1561abfcdd0819a",
            "tooltip": ""
          }
        },
        "9f4026c62c60493caa18c014ae414e65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a6ea48a80c194d128959422368aa0e10": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa11c10b4234452d9430744ab89b59a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a872283afaa4a33a9bc3f9e57b3650b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fe4d1052824c4ed29c38c5311087e650",
            "value": 1
          }
        },
        "acf3991e5d644f468264f561b28b514a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95160a05de5b402b9bdb0bd2d099cd00",
            "placeholder": "​",
            "style": "IPY_MODEL_cf376b0ea3544055b8867d7013526502",
            "value": "model.safetensors: 100%"
          }
        },
        "b011a6ccf8e745c6be6f3a17b7d61dce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_878dc96f87dd45f389948a546db33e94",
            "placeholder": "​",
            "style": "IPY_MODEL_006bf6f5ebaf400486a6b82610381db0",
            "value": ""
          }
        },
        "bfc4997e3bd94e66bebaa1ffdae1b99e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca05cbcd72cb41d9856804ffb17b26cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e283b1608c4d4266a03c57dc95aabb2e",
            "placeholder": "​",
            "style": "IPY_MODEL_bfc4997e3bd94e66bebaa1ffdae1b99e",
            "value": " 0/1 [00:00&lt;?, ?example/s]"
          }
        },
        "ca5804644ef345c1b4f670fb7f088fe8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc1f0bc4a1a74568b9c74a7392a1568f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc2a33e9a7ac4c5699346fcbf53b7c95": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "cc7d460d3c5a4ac6a9b64929736d6598": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cf376b0ea3544055b8867d7013526502": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d45a7d95e5b14a6f88d5798fec9c40a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e2c257232c3472697e03350de43cb30",
            "placeholder": "​",
            "style": "IPY_MODEL_381c780ce17e4662981175400fb8a0f8",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "d4acc9a7aca04564bfaefe33de079394": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "d6169577ffb341a69eb0175e301b6a44": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb1e19624fde4d3c8048ce00264d6056",
            "placeholder": "​",
            "style": "IPY_MODEL_9638a0456e0c41b1b9b9757932f55c53",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "e283b1608c4d4266a03c57dc95aabb2e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5c86e0e33264ed8b6325e44f9421653": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_acf3991e5d644f468264f561b28b514a",
              "IPY_MODEL_4a723c6a56e94309b2e3abe63f28aedc",
              "IPY_MODEL_ef25768d3b0746b48c6d02e9bd151bf9"
            ],
            "layout": "IPY_MODEL_cc1f0bc4a1a74568b9c74a7392a1568f"
          }
        },
        "ebe8aa7a82124b57bce2d826c1dea0fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_408f4dfce21a45cfad36047677ec8658",
            "placeholder": "​",
            "style": "IPY_MODEL_ca5804644ef345c1b4f670fb7f088fe8",
            "value": "Computing widget examples:   0%"
          }
        },
        "ef25768d3b0746b48c6d02e9bd151bf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13e9bf583f6442d395334947379a280a",
            "placeholder": "​",
            "style": "IPY_MODEL_1995b4d98fe044e38f1980d47033ca40",
            "value": " 1.34G/1.34G [01:11&lt;00:00, 21.0MB/s]"
          }
        },
        "fb1e19624fde4d3c8048ce00264d6056": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe4d1052824c4ed29c38c5311087e650": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
