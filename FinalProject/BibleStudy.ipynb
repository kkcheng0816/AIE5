{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7pQDUhUnIo8"
      },
      "source": [
        "## Final Project\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 1: Defining your Problem and Audience\n",
        "\n",
        "✅ Deliverables\n",
        "\n",
        "#### Problem Statement \n",
        "\n",
        "Christians often struggle with the Bible’s complex language, historical context, and interconnected passages. Traditional study tools lack immediacy and personalization. An LLM-powered Bible study tool addresses this by providing real-time, context-aware explanations, answering theological questions, and creating tailored resources. With interactive features like quizzes and personalized challenges, it enhances engagement and accessibility, helping believers deepen their faith effectively.\n",
        "\n",
        "Audience:\n",
        "2.6 billions of Christians in the world"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2: Propose a Solution\n",
        "\n",
        "✅ Deliverables\n",
        "\n",
        "#### Application Framework\n",
        "\n",
        "<img src=\"image/BibleStudy_Diagram.jpg\" />\n",
        "\n",
        "- LLM \n",
        "    - \"gpt-4o\" is used for agent reasoning\n",
        "    - \"gpt-4o-mini\" is used in the Bible rag_chain and quiz_question_generator\n",
        "- Embedding Model\n",
        "    -  the embedding model fine tuned for Genesis content and based on Snowflake/snowflake-arctic-embed-l (done in Midterm)\n",
        "- Orchestration\n",
        "    - There are two nodes in the graph: 1 agent node, 1 action node. The agent node does some reasoning based on the user queries and determines which tool in the action node to use. The action node provides a tool set for the agent node to use which includes\n",
        "        1. ai_rag_tool which answers the questions inside Bible\n",
        "        2. tavily_tool which searchs the topic on Internet\n",
        "        3. quiz_question_generator which generates interesting test questions based on verse range, so that this application may provide a quiz to test user's understanding on the verses \n",
        "- Vector Database\n",
        "    - Qdrant as it is a popular choirc for RAG. It is open sourced, excels at performing fast, accurate, and scalable vector similarity searches.\n",
        "- Monitoring\n",
        "    - LangSmith trace\n",
        "- User Interface\n",
        "    - Chainlit as it is open sourced and ease to use\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 3: Dealing with the Data\n",
        "\n",
        "✅ Deliverables\n",
        "1. Describe all of your data sources and external APIs, and describe what you’ll use them for.\n",
        "\n",
        "    - The book of Genesis is downloaded to local from https://www.vatican.va/archive/bible/genesis/documents/bible_genesis_en.html by using curl command. And then it is loaded by using langchain director loader.\n",
        "\n",
        "2. Describe the default chunking strategy that you will use.  Why did you make this decision?\n",
        "\n",
        "    - Instead of using RecursiveCharacterTextSplitter (which was used in Midterm),  `load_genesis_documents` function is used to load and parse HTML files containing Bible verses from the specified directory, extract verse content, chapter number and verse number from verses and create Document objects with metadata. \n",
        "    - Use same fine tuned embedding model `kcheng0816/finetuned_arctic_genesis`\n",
        "    - Convert Document objects into PointStruct objects with custom IDs (based on chapter and verse numbers), verse embeddings and payloads include the verse text and metadata, and upserted into the Qdrant collection.\n",
        "    - `retrieve_documents` function is used to retrieve documents from a Qdrant collection based on the input question. It first checks if the question contains a specific Bible verse reference(e.g., \"Genesis 1:1-5\"). If a reference is found, it retrieves the exact verses using `retrieve_verse_content` function. If no reference is found, it performs a semantic search using embeddings to find the most relevant documents.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 4: Building a Quick End-to-End Prototype\n",
        "\n",
        "✅ Deliverables\n",
        "\n",
        "https://huggingface.co/spaces/kcheng0816/BibleStudy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Installing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "!uv pip install -qU ragas==0.2.10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "!uv pip install -qU cohere langchain_cohere langchain_huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "!uv pip install -qU sentence_transformers datasets pyarrow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "!uv pip install -qU faiss-cpu python-pptx==1.0.2 nltk==3.9.1 beautifulsoup4 lxml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaVwN269EttM",
        "outputId": "ba50f775-3957-4d88-9a88-43acc6966dda"
      },
      "outputs": [],
      "source": [
        "!uv pip install -qU langchain-community==0.3.14 langchain-openai==0.2.14 unstructured==0.16.12 langgraph==0.2.61 langchain-qdrant==0.2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wujPjGJuoPwg"
      },
      "source": [
        "### Environment Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AIE5 - Bible Study Tool - 9d8a18ba\n"
          ]
        }
      ],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE5 - Bible Study Tool - {uuid4().hex[0:8]}\"\n",
        "print(os.environ[\"LANGCHAIN_PROJECT\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!curl https://www.vatican.va/archive/bible/genesis/documents/bible_genesis_en.html -o data/bible_genesis_en.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.docstore.document import Document\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "\n",
        "path = \"data/\"\n",
        "book = \"Genesis\"\n",
        "collection_name = \"genesis_study\"\n",
        "# Parse the html file and Load Genesis documents \n",
        "def load_genesis_documents(path, book_name):\n",
        "    \"\"\"\n",
        "    Load and parse HTML files containing Bible verses from the specified directory.\n",
        "    Extracts verses from Genesis and creates Document objects with metadata.\n",
        "\n",
        "    Args:\n",
        "        path (str): The directory path containing HTML files with Bible verses.\n",
        "        book_name (str): The name of the book (e.g., \"Genesis\").\n",
        "\n",
        "    Returns:\n",
        "        list[Document]: A list of Document objects, each containing a verse's text and metadata.\n",
        "    \"\"\"\n",
        "    documents = []\n",
        "    for file in os.listdir(path):\n",
        "        if file.endswith(\".html\"):\n",
        "            file_path = os.path.join(path, file)\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                soup = BeautifulSoup(f, \"html.parser\")\n",
        "                p_tags = soup.find_all(\"p\", align=\"left\")\n",
        "                for p_tag in p_tags:\n",
        "                    verse_texts = [content.strip() for content in p_tag.contents \n",
        "                                   if isinstance(content, str) and content.strip()]\n",
        "                    for verse in verse_texts:\n",
        "                        match = re.match(r\"\\[(\\d+):(\\d+)\\]\\s*(.*)\", verse)\n",
        "                        if match:\n",
        "                            chapter = int(match.group(1))\n",
        "                            verse_num = int(match.group(2))\n",
        "                            text = match.group(3)\n",
        "                            doc = Document(\n",
        "                                page_content=text,\n",
        "                                metadata={\"book\": book_name, \"chapter\": chapter, \"verse\": verse_num}\n",
        "                            )\n",
        "                            documents.append(doc)\n",
        "    return documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1533\n",
            "[Document(metadata={'book': 'Genesis', 'chapter': 34, 'verse': 20}, page_content='So Hamor and his son Shechem came to the gate of their city and spoke to the men of their city, saying,'), Document(metadata={'book': 'Genesis', 'chapter': 34, 'verse': 21}, page_content='\"These people are friendly with us; let them live in the land and trade in it, for the land is large enough for them; let us take their daughters in marriage, and let us give them our daughters.'), Document(metadata={'book': 'Genesis', 'chapter': 34, 'verse': 22}, page_content='Only on this condition will they agree to live among us, to become one people: that every male among us be circumcised as they are circumcised.'), Document(metadata={'book': 'Genesis', 'chapter': 34, 'verse': 23}, page_content='Will not their livestock, their property, and all their animals be ours? Only let us agree with them, and they will live among us.\"'), Document(metadata={'book': 'Genesis', 'chapter': 34, 'verse': 24}, page_content='And all who went out of the city gate heeded Hamor and his son Shechem; and every male was circumcised, all who went out of the gate of his city.'), Document(metadata={'book': 'Genesis', 'chapter': 34, 'verse': 25}, page_content=\"On the third day, when they were still in pain, two of the sons of Jacob, Simeon and Levi, Dinah's brothers, took their swords and came against the city unawares, and killed all the males.\"), Document(metadata={'book': 'Genesis', 'chapter': 34, 'verse': 26}, page_content=\"They killed Hamor and his son Shechem with the sword, and took Dinah out of Shechem's house, and went away.\"), Document(metadata={'book': 'Genesis', 'chapter': 34, 'verse': 27}, page_content='And the other sons of Jacob came upon the slain, and plundered the city, because their sister had been defiled.'), Document(metadata={'book': 'Genesis', 'chapter': 34, 'verse': 28}, page_content='They took their flocks and their herds, their donkeys, and whatever was in the city and in the field.'), Document(metadata={'book': 'Genesis', 'chapter': 34, 'verse': 29}, page_content='All their wealth, all their little ones and their wives, all that was in the houses, they captured and made their prey.'), Document(metadata={'book': 'Genesis', 'chapter': 34, 'verse': 30}, page_content='Then Jacob said to Simeon and Levi, \"You have brought trouble on me by making me odious to the inhabitants of the land, the Canaanites and the Perizzites; my numbers are few, and if they gather themselves against me and attack me, I shall be destroyed, both I and my household.\"'), Document(metadata={'book': 'Genesis', 'chapter': 34, 'verse': 31}, page_content='But they said, \"Should our sister be treated like a whore?\"'), Document(metadata={'book': 'Genesis', 'chapter': 35, 'verse': 1}, page_content='God said to Jacob, \"Arise, go up to Bethel, and settle there. Make an altar there to the God who appeared to you when you fled from your brother Esau.\"'), Document(metadata={'book': 'Genesis', 'chapter': 35, 'verse': 2}, page_content='So Jacob said to his household and to all who were with him, \"Put away the foreign gods that are among you, and purify yourselves, and change your clothes;'), Document(metadata={'book': 'Genesis', 'chapter': 35, 'verse': 3}, page_content='then come, let us go up to Bethel, that I may make an altar there to the God who answered me in the day of my distress and has been with me wherever I have gone.\"')]\n"
          ]
        }
      ],
      "source": [
        "documents = load_genesis_documents(\"data/\", \"Genesis\")\n",
        "print(len(documents))\n",
        "print(documents[1000:1015])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Embedding Model and Vector Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/llmops-course/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Some weights of BertModel were not initialized from the model checkpoint at kcheng0816/finetuned_arctic_genesis and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http.models import VectorParams, Distance\n",
        "from qdrant_client.http.models import PointStruct\n",
        "import uuid\n",
        "\n",
        "# Initialize embeddings using a fine-tuned model from HuggingFace\n",
        "# The model \"kcheng0816/finetuned_arctic_genesis\" is tailored for Genesis-related content, ideal for a Bible Study tool\n",
        "huggingface_embeddings = HuggingFaceEmbeddings(model_name=\"kcheng0816/finetuned_arctic_genesis\")\n",
        "# This dynamically determines the vector size based on the model's output, ensuring compatibility\n",
        "dimension = len(huggingface_embeddings.embed_query(\"test\"))\n",
        "\n",
        "# Set up an in-memory Qdrant client\n",
        "# Qdrant is a vector database used here to store and retrieve embeddings efficiently in memory\n",
        "client = QdrantClient(\":memory:\")\n",
        "\n",
        "# Create a new collection in Qdrant to store the embeddings\n",
        "# 'collection_name' (e.g., \"genesis_study\") names the collection; VectorParams sets the vector size and uses cosine similarity for distance measurement\n",
        "client.create_collection(\n",
        "    collection_name=collection_name,\n",
        "    vectors_config=VectorParams(size=dimension, distance=Distance.COSINE)\n",
        ")\n",
        "\n",
        "# Prepare a list of PointStruct objects for uploading to Qdrant\n",
        "# Each PointStruct pairs an embedding with its document's metadata for storage and retrieval\n",
        "embeddings = huggingface_embeddings.embed_documents([doc.page_content for doc in documents])\n",
        "points = [\n",
        "    PointStruct(\n",
        "        # Generate a unique ID using UUID5 based on chapter and verse\n",
        "        # This ensures consistent, reproducible IDs for each verse across different runs\n",
        "        id=str(uuid.uuid5(uuid.NAMESPACE_DNS, f\"{doc.metadata['chapter']}_{doc.metadata['verse']}\")),\n",
        "        vector=embedding,\n",
        "        payload={\n",
        "            \"text\": doc.page_content,\n",
        "            \"book\": doc.metadata[\"book\"],\n",
        "            \"chapter\": doc.metadata[\"chapter\"],\n",
        "            \"verse\": doc.metadata[\"verse\"]\n",
        "        }\n",
        "    )\n",
        "    for embedding, doc in zip(embeddings, documents)\n",
        "]\n",
        "client.upsert(collection_name=collection_name, points=points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([Record(id='25277d86-b537-5f22-9a56-3da3aee3d9b8', payload={'text': 'In the beginning when God created the heavens and the earth,', 'book': 'Genesis', 'chapter': 1, 'verse': 1}, vector=None, shard_key=None, order_value=None),\n",
              "  Record(id='932ab876-7702-5073-a0a1-a3173a25231c', payload={'text': 'the earth was a formless void and darkness covered the face of the deep, while a wind from God swept over the face of the waters.', 'book': 'Genesis', 'chapter': 1, 'verse': 2}, vector=None, shard_key=None, order_value=None),\n",
              "  Record(id='bae05c86-bd23-5f55-b1ae-7ce063a0f460', payload={'text': 'Then God said, \"Let there be light\"; and there was light.', 'book': 'Genesis', 'chapter': 1, 'verse': 3}, vector=None, shard_key=None, order_value=None)],\n",
              " None)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from qdrant_client.http.models import Filter, FieldCondition, MatchValue, MatchAny\n",
        "\n",
        "filter = Filter(\n",
        "        must=[\n",
        "            FieldCondition(key=\"book\", match=MatchValue(value=\"Genesis\")),\n",
        "            FieldCondition(key=\"chapter\", match=MatchValue(value=1)),\n",
        "            FieldCondition(key=\"verse\", match=MatchAny(any=[1,2,3]))\n",
        "        ]\n",
        "    )\n",
        "search_result = client.scroll(\n",
        "        collection_name=\"genesis_study\", \n",
        "        scroll_filter=filter,\n",
        "        limit=3\n",
        "    )\n",
        "search_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http.models import Filter, FieldCondition, MatchValue, MatchAny\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "def parse_verse_reference(ref: str):\n",
        "    \"\"\"\n",
        "    Parse a verse reference string into book, chapter, and a list of verse numbers.\n",
        "    \n",
        "    Args:\n",
        "        ref (str): The verse reference, e.g., \"Genesis 1:1-10\".\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (book, chapter, verses) where verses is a list of integers, or None if invalid.\n",
        "    \"\"\"\n",
        "    match = re.match(r\"(\\w+(?:\\s\\w+)?)\\s(\\d+):([\\d,-]+)\", ref)\n",
        "    if not match:\n",
        "        return None\n",
        "    book, chapter, verse_part = match.groups()\n",
        "    chapter = int(chapter)\n",
        "    verses = []\n",
        "    for part in verse_part.split(','):\n",
        "        if '-' in part:\n",
        "            start, end = map(int, part.split('-'))\n",
        "            verses.extend(range(start, end + 1))\n",
        "        else:\n",
        "            verses.append(int(part))\n",
        "    return book, chapter, verses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieve_verse_content(verse_range: str, client: QdrantClient):\n",
        "    \"\"\"\n",
        "    Retrieve Bible verses from Qdrant based on the specified verse range.\n",
        "\n",
        "    Parameters:\n",
        "    - verse_range (str): The verse range in the format \"Book Chapter:Verses\", e.g., \"Genesis 1:1-5\".\n",
        "    - client (QdrantClient): The Qdrant client to query the database.\n",
        "\n",
        "    Returns:\n",
        "    - list[Document]: A list of Document objects containing the verse text and metadata.\n",
        "    - str: An error message if the verse range is invalid or no verses are found.\n",
        "    \"\"\"\n",
        "    # Parse the verse range into book, chapter, and verses\n",
        "    parsed = parse_verse_reference(verse_range)\n",
        "    if not parsed:\n",
        "        return \"Invalid verse range format.\"\n",
        "    book, chapter, verses = parsed\n",
        "\n",
        "    # Create a filter for Qdrant to match the specified book, chapter, and verses\n",
        "    filter = Filter(\n",
        "        must=[\n",
        "            FieldCondition(key=\"book\", match=MatchValue(value=book)),\n",
        "            FieldCondition(key=\"chapter\", match=MatchValue(value=chapter)),\n",
        "            FieldCondition(key=\"verse\", match=MatchAny(any=verses))\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Retrieve the verses from Qdrant using the filter\n",
        "    search_result = client.scroll(\n",
        "        collection_name=collection_name,\n",
        "        scroll_filter=filter,\n",
        "        limit=len(verses)\n",
        "    )\n",
        "    if not search_result[0]:\n",
        "        return \"No verses found for the specified range.\"\n",
        "    \n",
        "    # Sort the retrieved points by verse number to ensure sequential order\n",
        "    sorted_points = sorted(search_result[0], key=lambda p: p.payload[\"verse\"])\n",
        "\n",
        "    # Create Document objects from the sorted points\n",
        "    docs = [\n",
        "        Document(\n",
        "            page_content=p.payload[\"text\"],\n",
        "            metadata=p.payload\n",
        "        )\n",
        "        for p in sorted_points\n",
        "    ]\n",
        "    return docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'text': 'In the beginning when God created the heavens and the earth,', 'book': 'Genesis', 'chapter': 1, 'verse': 1}, page_content='In the beginning when God created the heavens and the earth,'),\n",
              " Document(metadata={'text': 'the earth was a formless void and darkness covered the face of the deep, while a wind from God swept over the face of the waters.', 'book': 'Genesis', 'chapter': 1, 'verse': 2}, page_content='the earth was a formless void and darkness covered the face of the deep, while a wind from God swept over the face of the waters.'),\n",
              " Document(metadata={'text': 'Then God said, \"Let there be light\"; and there was light.', 'book': 'Genesis', 'chapter': 1, 'verse': 3}, page_content='Then God said, \"Let there be light\"; and there was light.')]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs = retrieve_verse_content(\"Genesis 1:1-3\", client)\n",
        "docs\n",
        "# Output: List of Document objects for Genesis 1:1, 1:2, 1:3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieve_documents(question: str, client: QdrantClient):\n",
        "    \"\"\"\n",
        "    Retrieve documents from a Qdrant collection based on the input question.\n",
        "\n",
        "    This function first checks if the question contains a specific Bible verse reference\n",
        "    (e.g., \"Genesis 1:1-5\"). If a reference is found, it retrieves the exact verses using\n",
        "    `retrieve_verse_content`. If no reference is found, it performs a semantic search\n",
        "    using embeddings to find the most relevant documents.\n",
        "\n",
        "    Parameters:\n",
        "    - question (str): The input question or query string.\n",
        "    - collection_name (str): The name of the Qdrant collection to search in.\n",
        "    - client (QdrantClient): The Qdrant client object used to interact with the database.\n",
        "\n",
        "    Returns:\n",
        "    - list[Document]: A list of Document objects containing the relevant verse text and metadata.\n",
        "    - str: An error message if no relevant documents are found or if the verse reference is invalid.\n",
        "    \"\"\"\n",
        "    reference_match = re.search(r\"(\\w+)\\s?(\\d+):\\s?([\\d,-]+)\", question)\n",
        "    if reference_match:\n",
        "        verse_range = reference_match.group(1) + ' ' + reference_match.group(2) + ':' + reference_match.group(3)\n",
        "        return retrieve_verse_content(verse_range, client)\n",
        "    else:\n",
        "        query_vector = huggingface_embeddings.embed_query(question)\n",
        "        search_result = client.query_points(\n",
        "            collection_name=collection_name,\n",
        "            query=query_vector,\n",
        "            limit=5,\n",
        "            with_payload=True\n",
        "        ).points\n",
        "        if search_result:\n",
        "            return [\n",
        "                Document(\n",
        "                    page_content=point.payload[\"text\"],\n",
        "                    metadata=point.payload\n",
        "                )\n",
        "                for point in search_result\n",
        "            ]\n",
        "        return \"No relevant documents found.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'text': 'So God blessed the seventh day and hallowed it, because on it God rested from all the work that he had done in creation.', 'book': 'Genesis', 'chapter': 2, 'verse': 3}, page_content='So God blessed the seventh day and hallowed it, because on it God rested from all the work that he had done in creation.'),\n",
              " Document(metadata={'text': 'These are the generations of the heavens and the earth when they were created. In the day that the LORD God made the earth and the heavens,', 'book': 'Genesis', 'chapter': 2, 'verse': 4}, page_content='These are the generations of the heavens and the earth when they were created. In the day that the LORD God made the earth and the heavens,'),\n",
              " Document(metadata={'text': 'when no plant of the field was yet in the earth and no herb of the field had yet sprung up - for the LORD God had not caused it to rain upon the earth, and there was no one to till the ground;', 'book': 'Genesis', 'chapter': 2, 'verse': 5}, page_content='when no plant of the field was yet in the earth and no herb of the field had yet sprung up - for the LORD God had not caused it to rain upon the earth, and there was no one to till the ground;')]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs = retrieve_documents(\"Genesis 2:3-5\", client)\n",
        "docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Augmented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_PROMPT = \"\"\"\\\n",
        "You are a helpful assistant who answers questions based on provided context. You must only use the provided context, and cannot use your own knowledge.\n",
        "\n",
        "### Question\n",
        "{question}\n",
        "\n",
        "### Context\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
        "\n",
        "rate_limiter = InMemoryRateLimiter(\n",
        "    requests_per_second=1,  # <-- make a request once every 1 seconds!!\n",
        "    check_every_n_seconds=0.1,  # Wake up every 100 ms to check whether allowed to make a request,\n",
        "    max_bucket_size=10,  # Controls the maximum burst size.\n",
        ")\n",
        "\n",
        "chat_model = init_chat_model(\"gpt-4o-mini\", rate_limiter=rate_limiter)\n",
        "\n",
        "from langchain_core.globals import set_llm_cache\n",
        "from langchain_core.caches import InMemoryCache\n",
        "set_llm_cache(InMemoryCache())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### RAG Chain ####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# Factory function to create the Runnable\n",
        "def create_retriever_runnable(client: QdrantClient) -> RunnableLambda:\n",
        "    return RunnableLambda(lambda question: retrieve_documents(question, client))\n",
        "\n",
        "retrieval_runnable = create_retriever_runnable(client)\n",
        "\n",
        "def format_docs(docs):\n",
        "    if isinstance(docs, str):  # Handle error message case\n",
        "        return docs\n",
        "    return \"\\n\\n\".join(f\"Genesis {doc.metadata['chapter']}:{doc.metadata['verse']} - {doc.page_content}\" for doc in docs)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retrieval_runnable | RunnableLambda(format_docs), \"question\": RunnablePassthrough()}\n",
        "    | RunnablePassthrough.assign(response=rag_prompt | chat_model | StrOutputParser())\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First call to LLM -- time without cache: 6.2720 seconds\n",
            "Second call to LLM -- time with cache: 0.1089 seconds\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "def test_rag_llm_cache(question):\n",
        "    start_time = time.time()\n",
        "    rag_chain.invoke(question)\n",
        "    time_no_cache = time.time() - start_time\n",
        "\n",
        "    start_time = time.time()\n",
        "    rag_chain.invoke(question)\n",
        "    time_with_cache = time.time() - start_time\n",
        "\n",
        "    print(f\"First call to LLM -- time without cache: {time_no_cache:.4f} seconds\")\n",
        "    print(f\"Second call to LLM -- time with cache: {time_with_cache:.4f} seconds\")\n",
        "\n",
        "test_rag_llm_cache(\"How did GOD create the whole universe in Genesis? How many days did GOD take and what did GOD do in each day\")\n",
        " # Prints the context (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Genesis 3:16-17 describes the consequences of the actions taken by the first man and woman in the Garden of Eden, specifically their disobedience to God's command.\n",
            "\n",
            "In Genesis 3:16, God speaks to the woman and outlines two key consequences of her actions. First, He states that He will greatly increase her pain in childbearing, indicating that childbirth will involve significant suffering. Second, He notes that while she will have a desire for her husband, he will have dominion over her, implying a change in the dynamics of their relationship where the man will hold authority.\n",
            "\n",
            "In Genesis 3:17, God then addresses the man, telling him that because he listened to his wife and ate from the forbidden tree, the ground is cursed as a result. This curse means that he will have to work the land with great difficulty, experiencing toil and hardship in providing for himself throughout his life.\n",
            "\n",
            "Together, these verses illustrate the repercussions of human disobedience, affecting both men and women in distinct ways and reshaping their roles and experiences in the world.\n"
          ]
        }
      ],
      "source": [
        "response = rag_chain.invoke(\"Could you please explain Genesis 3:16-17?\")\n",
        "print(response[\"response\"]) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ai_rag_tool for answering bible questions ####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "def format_contexts(docs):\n",
        "    return \"\\n\\n\".join(docs) if isinstance(docs, list) else docs\n",
        "\n",
        "@tool\n",
        "def ai_rag_tool(question: str):\n",
        "    \"\"\"Useful for when you need to answer questions about Bible \"\"\"\n",
        "    response = rag_chain.invoke(question)\n",
        "    return {\n",
        "        \"message\": [HumanMessage(content=response[\"response\"])],\n",
        "        \"context\": format_contexts(response[\"context\"])\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = ai_rag_tool(\"How did GOD create the whole universe in Genesis? How many days did GOD take and what did GOD do in each day?\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### tavily_tool for searching more information on Internet ####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "tavily_tool = TavilySearchResults(max_results=5)   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Bible Quiz Question Generator ####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _generate_quiz_question(verse_range: str, client: QdrantClient):\n",
        "    docs = retrieve_verse_content(verse_range, client)\n",
        "    if isinstance(docs, str):\n",
        "        return {\"error\": docs}\n",
        "    \n",
        "    # Randomly select a subset of verses if the range has more than 3 verses\n",
        "    num_verses = len(docs)\n",
        "    if num_verses > 3:\n",
        "        subset_size = random.randint(1, 3)\n",
        "        start_idx = random.randint(0, num_verses - subset_size)\n",
        "        selected_docs = docs[start_idx : start_idx + subset_size]\n",
        "    else:\n",
        "        selected_docs = docs\n",
        "    \n",
        "    verse_content = \"\\n\".join(\n",
        "        f\"{doc.metadata['book']} {doc.metadata['chapter']}:{doc.metadata['verse']} - {doc.page_content}\"\n",
        "        for doc in selected_docs\n",
        "    )\n",
        "    \n",
        "    quiz_prompt = ChatPromptTemplate.from_template(\n",
        "        \"Based on the following Bible verse(s), generate a multiple-choice quiz question with 4 options (A, B, C, D) \"\n",
        "        \"and indicate the correct answer:\\n\\n\"\n",
        "        \"{verse_content}\\n\\n\"\n",
        "        \"Format your response as follows:\\n\"\n",
        "        \"Question: [Your question here]\\n\"\n",
        "        \"A: [Option A]\\n\"\n",
        "        \"B: [Option B]\\n\"\n",
        "        \"C: [Option C]\\n\"\n",
        "        \"D: [Option D]\\n\"\n",
        "        \"Correct Answer: [Letter of correct answer]\\n\"\n",
        "        \"Explanation: [Brief explanation of why the answer is correct]\\n\"\n",
        "    )\n",
        "    \n",
        "    # Use a higher temperature for more diverse question generation\n",
        "    chat_model_with_temp = chat_model.bind(temperature=0.9)\n",
        "    response = (quiz_prompt | chat_model_with_temp).invoke({\"verse_content\": verse_content})\n",
        "    \n",
        "    response_text = response.content.strip()\n",
        "    lines = response_text.split(\"\\n\")\n",
        "    question = \"\"\n",
        "    options = {}\n",
        "    correct_answer = \"\"\n",
        "    explanation = \"\"\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line.startswith(\"Question:\"):\n",
        "            question = line[len(\"Question:\"):].strip()\n",
        "        elif line.startswith((\"A:\", \"B:\", \"C:\", \"D:\")):\n",
        "            key, value = line.split(\":\", 1)\n",
        "            options[key.strip()] = value.strip()\n",
        "        elif line.startswith(\"Correct Answer:\"):\n",
        "            correct_answer = line[len(\"Correct Answer:\"):].strip()\n",
        "        elif line.startswith(\"Explanation:\"):\n",
        "            explanation = line[len(\"Explanation:\"):].strip()\n",
        "    \n",
        "    return {\n",
        "        \"quiz_question\": question,\n",
        "        \"options\": options,\n",
        "        \"correct_answer\": correct_answer,\n",
        "        \"explanation\": explanation,\n",
        "        \"verse_range\": verse_range,\n",
        "        \"verse_content\": verse_content\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "question = _generate_quiz_question(\"Genesis 1:1-5\", client)\n",
        "question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from functools import partial\n",
        "\n",
        "generate_quiz_question_tool = partial(_generate_quiz_question, client=client)\n",
        "\n",
        "@tool\n",
        "def quiz_question_generator(verse_range: str):\n",
        "    \"\"\"Generate a quiz question based on the content of the specified verse range.\"\"\"\n",
        "    quiz_data = generate_quiz_question_tool(verse_range)\n",
        "    return json.dumps(quiz_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tool Belt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "tool_belt = [\n",
        "    ai_rag_tool,\n",
        "    tavily_tool,\n",
        "    quiz_question_generator\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI-C669ZYVI5"
      },
      "source": [
        "### LangGraph Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Binding the tools to the LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "QkNS8rNZJs4z"
      },
      "outputs": [],
      "source": [
        "llm = init_chat_model(\"gpt-4o\", temperature=0, rate_limiter=rate_limiter)\n",
        "llm_with_tools = llm.bind_tools(tool_belt)\n",
        "\n",
        "from langchain_core.globals import set_llm_cache\n",
        "from langchain_core.caches import InMemoryCache\n",
        "set_llm_cache(InMemoryCache())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define the State"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Optional, List, Annotated, TypedDict\n",
        "from langchain_core.messages import AnyMessage\n",
        "from langchain_core.documents import Document\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[list[AnyMessage], add_messages]\n",
        "    in_quiz: bool\n",
        "    quiz_question: Optional[dict]\n",
        "    verse_range: Optional[str]\n",
        "    quiz_score: int\n",
        "    quiz_total: int\n",
        "    waiting_for_answer: bool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_X4ff3GAzeCOVAVdW2SvmEQLy', 'function': {'arguments': '{\"verse_range\":\"Genesis 1:1-10\"}', 'name': 'generate_quiz_question'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 148, 'total_tokens': 172, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f9f4fb6dbf', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-99c9cee6-2612-45f5-af7a-03db477dc2a9-0', tool_calls=[{'name': 'generate_quiz_question', 'args': {'verse_range': 'Genesis 1:1-10'}, 'id': 'call_X4ff3GAzeCOVAVdW2SvmEQLy', 'type': 'tool_call'}], usage_metadata={'input_tokens': 148, 'output_tokens': 24, 'total_tokens': 172, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, ToolMessage\n",
        "\n",
        "response = llm_with_tools.invoke([AIMessage(content=f\"Generate quiz question for Genesis 1:1-10\")])\n",
        "response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First call to LLM -- time without cache: 0.8739 seconds\n",
            "Second call to LLM -- time with cache: 0.0010 seconds\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "def test_llm_cache(question):\n",
        "    start_time = time.time()\n",
        "    llm_with_tools.invoke([AIMessage(content=question)])\n",
        "    time_no_cache = time.time() - start_time\n",
        "\n",
        "    start_time = time.time()\n",
        "    llm_with_tools.invoke([AIMessage(content=question)])\n",
        "    time_with_cache = time.time() - start_time\n",
        "\n",
        "    print(f\"First call to LLM -- time without cache: {time_no_cache:.4f} seconds\")\n",
        "    print(f\"Second call to LLM -- time with cache: {time_with_cache:.4f} seconds\")\n",
        "\n",
        "test_llm_cache(\"Could you please explain Genesis 3:15-16?\")\n",
        " # Prints the context (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# System message\n",
        "system_message = SystemMessage(content=\"\"\"You are a Bible study assistant. You can answer questions about the Bible, search the internet for related information, or generate quiz questions based on specific verse ranges.\n",
        "\n",
        "- Use the 'ai_rag_tool' to answer questions about the Bible.\n",
        "- Use the 'tavily_tool' to search the internet for additional information.\n",
        "- Use the 'quiz_question_generator' tool when the user requests to start a quiz on a specific verse range, such as 'start quiz on Genesis 1:1-10'.\n",
        "\n",
        "When the user requests a quiz, extract the verse range from their message and pass it to the 'quiz_question_generator' tool.\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Agent Function ####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Agent function\n",
        "def call_mode(state):\n",
        "    \"\"\"\n",
        "    Manage the conversation flow of the Bible Study Tool, focusing on quiz mode and regular interactions.\n",
        "\n",
        "    This function determines the next action in the conversation based on the user's input and the current state.\n",
        "    It handles quiz mode (processing answers, continuing or ending the quiz) and transitions to or from regular\n",
        "    question-answering mode. It also processes tool calls, such as starting a quiz, and delegates non-quiz queries\n",
        "    to a language model.\n",
        "\n",
        "    Parameters:\n",
        "    - state (dict): The current state of the conversation, containing messages, quiz status, and other data.\n",
        "\n",
        "    Returns:\n",
        "    - dict: An updated state dictionary with new messages and modified quiz-related fields as needed.\n",
        "    \"\"\"\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    \n",
        "    if state.get(\"in_quiz\", False):\n",
        "        if state.get(\"waiting_for_answer\", False):\n",
        "            # Process the user's answer\n",
        "            quiz_data = state[\"quiz_question\"]\n",
        "            user_answer = last_message.content.strip().upper()\n",
        "            correct_answer = quiz_data[\"correct_answer\"]\n",
        "            new_quiz_total = state[\"quiz_total\"] + 1\n",
        "            if user_answer == correct_answer:\n",
        "                new_quiz_score = state[\"quiz_score\"] + 1\n",
        "                feedback = f\"Correct! {quiz_data['explanation']}\"\n",
        "            else:\n",
        "                new_quiz_score = state[\"quiz_score\"]\n",
        "                feedback = f\"Incorrect. The correct answer is {correct_answer}. {quiz_data['explanation']}\"\n",
        "            return {\n",
        "                \"messages\": [\n",
        "                    AIMessage(content=feedback),\n",
        "                    AIMessage(content=\"Would you like another question? Type 'Yes' to continue or 'No' to end the quiz.\")\n",
        "                ],\n",
        "                \"quiz_total\": new_quiz_total,\n",
        "                \"quiz_score\": new_quiz_score,\n",
        "                \"waiting_for_answer\": False,\n",
        "                \"quiz_question\": state[\"quiz_question\"],\n",
        "                \"in_quiz\": True,\n",
        "                \"verse_range\": state[\"verse_range\"]\n",
        "            }\n",
        "        else:\n",
        "            # Handle the user's decision to continue or stop the quiz\n",
        "            user_input = last_message.content.strip().lower()\n",
        "            if user_input == \"yes\":\n",
        "                # Generate a new quiz question\n",
        "                verse_range = state[\"verse_range\"]\n",
        "                quiz_data_str = quiz_question_generator(verse_range)\n",
        "                quiz_data = json.loads(quiz_data_str)\n",
        "                question = quiz_data[\"quiz_question\"]\n",
        "                options = \"\\n\".join([f\"{k}: {v}\" for k, v in quiz_data[\"options\"].items()])\n",
        "                verse_content = quiz_data[\"verse_content\"]\n",
        "                message_to_user = (\n",
        "                    f\"Based on the following verse(s):\\n\\n{verse_content}\\n\\n\"\n",
        "                    f\"Here's your quiz question:\\n\\n{question}\\n\\n{options}\\n\\n\"\n",
        "                    \"Please select your answer (A, B, C, or D).\"\n",
        "                )\n",
        "                return {\n",
        "                    \"messages\": [AIMessage(content=message_to_user)],\n",
        "                    \"quiz_question\": quiz_data,\n",
        "                    \"waiting_for_answer\": True,\n",
        "                    \"quiz_total\": state[\"quiz_total\"],\n",
        "                    \"quiz_score\": state[\"quiz_score\"],\n",
        "                    \"in_quiz\": True,\n",
        "                    \"verse_range\": state[\"verse_range\"]\n",
        "                }\n",
        "            elif user_input == \"no\":\n",
        "                # End the quiz and provide a summary\n",
        "                score = state[\"quiz_score\"]\n",
        "                total = state[\"quiz_total\"]\n",
        "                continue_message = \"Ask me anything about Genesis or type 'start quiz on <verse range>' (e.g., 'start quiz on Genesis 1:1-5') for a trivia challenge.\"\n",
        "                if total > 0:\n",
        "                    percentage = (score / total) * 100\n",
        "                    if percentage == 100:\n",
        "                        feedback = \"Excellent! You got all questions correct. Please continue your Bible study!\"\n",
        "                    elif percentage >= 80:\n",
        "                        feedback = \"Great job! You have a strong understanding. Please continue your Bible study!\"\n",
        "                    elif percentage >= 50:\n",
        "                        feedback = \"Good effort! Keep practicing to improve. Please continue your Bible study!\"\n",
        "                    else:\n",
        "                        feedback = \"Don’t worry, keep your Bible studying and you’ll get better!\"\n",
        "                    summary = f\"You got {score} out of {total} questions correct. {feedback} \\n\\n {continue_message}\"\n",
        "                else:\n",
        "                    summary = \"No questions were attempted.\"\n",
        "                return {\n",
        "                    \"messages\": [AIMessage(content=summary)],\n",
        "                    \"in_quiz\": False,\n",
        "                    \"quiz_question\": None,\n",
        "                    \"verse_range\": None,\n",
        "                    \"quiz_score\": 0,\n",
        "                    \"quiz_total\": 0,\n",
        "                    \"waiting_for_answer\": False\n",
        "                }\n",
        "            else:\n",
        "                # Handle invalid input\n",
        "                return {\n",
        "                    \"messages\": [AIMessage(content=\"Please type 'Yes' to continue or 'No' to end the quiz.\")],\n",
        "                    \"quiz_total\": state[\"quiz_total\"],\n",
        "                    \"quiz_score\": state[\"quiz_score\"],\n",
        "                    \"waiting_for_answer\": False,\n",
        "                    \"quiz_question\": state[\"quiz_question\"],\n",
        "                    \"in_quiz\": True,\n",
        "                    \"verse_range\": state[\"verse_range\"]\n",
        "                }\n",
        "    \n",
        "    # Handle starting the quiz or other tool calls\n",
        "    if len(state[\"messages\"]) >= 2 and isinstance(last_message, ToolMessage):\n",
        "        prev_message = state[\"messages\"][-2]\n",
        "        if isinstance(prev_message, AIMessage) and prev_message.tool_calls:\n",
        "            tool_call = prev_message.tool_calls[0]\n",
        "            if tool_call[\"name\"] == \"quiz_question_generator\":\n",
        "                # Start the quiz\n",
        "                quiz_data_str = last_message.content\n",
        "                quiz_data = json.loads(quiz_data_str)\n",
        "                verse_range = quiz_data[\"verse_range\"]\n",
        "                question = quiz_data[\"quiz_question\"]\n",
        "                options = \"\\n\".join([f\"{k}: {v}\" for k, v in quiz_data[\"options\"].items()])\n",
        "                verse_content = quiz_data[\"verse_content\"]\n",
        "                message_to_user = (\n",
        "                    f\"Based on the following verse(s):\\n\\n{verse_content}\\n\\n\"\n",
        "                    f\"Here's your quiz question:\\n\\n{question}\\n\\n{options}\\n\\n\"\n",
        "                    \"Please select your answer (A, B, C, or D).\"\n",
        "                )\n",
        "                return {\n",
        "                    \"messages\": [AIMessage(content=message_to_user)],\n",
        "                    \"in_quiz\": True,\n",
        "                    \"verse_range\": verse_range,\n",
        "                    \"quiz_score\": 0,\n",
        "                    \"quiz_total\": 0,\n",
        "                    \"quiz_question\": quiz_data,\n",
        "                    \"waiting_for_answer\": True\n",
        "                }\n",
        "    \n",
        "    # Process regular questions or commands\n",
        "    messages = [system_message] + state[\"messages\"]\n",
        "    response = llm_with_tools.invoke(messages)\n",
        "    return {\"messages\": [response]}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolNode\n",
        "tool_node = ToolNode(tool_belt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.graph import END\n",
        "#edge function\n",
        "def should_continue(state):\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    if last_message.tool_calls:\n",
        "        return \"action\"\n",
        "    return END"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Building the graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "from IPython.display import Image, display\n",
        "\n",
        "uncompiled_graph = StateGraph(AgentState)\n",
        "\n",
        "uncompiled_graph.add_node(\"agent\", call_mode)\n",
        "uncompiled_graph.add_node(\"action\", tool_node)\n",
        "\n",
        "uncompiled_graph.set_entry_point(\"agent\")\n",
        "\n",
        "uncompiled_graph.add_conditional_edges(\"agent\", should_continue)\n",
        "uncompiled_graph.add_edge(\"action\", \"agent\")\n",
        "\n",
        "compiled_graph = uncompiled_graph.compile()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_sGb7YDbb41PSJdJaZVzt5ojz', 'function': {'arguments': '{\"question\":\"How many days did God take to create the universe according to the Bible?\"}', 'name': 'ai_rag_tool'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 279, 'total_tokens': 310, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f9f4fb6dbf', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-df36c845-5b97-4d7a-853c-4e5a6022d318-0', tool_calls=[{'name': 'ai_rag_tool', 'args': {'question': 'How many days did God take to create the universe according to the Bible?'}, 'id': 'call_sGb7YDbb41PSJdJaZVzt5ojz', 'type': 'tool_call'}], usage_metadata={'input_tokens': 279, 'output_tokens': 31, 'total_tokens': 310, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "Tool Used: ai_rag_tool\n",
            "[ToolMessage(content=\"{'message': [HumanMessage(content='According to the Bible, God took six days to create the universe and then rested on the seventh day.', additional_kwargs={}, response_metadata={})], 'context': 'Genesis 1:1 - In the beginning when God created the heavens and the earth,\\\\n\\\\nGenesis 2:4 - These are the generations of the heavens and the earth when they were created. In the day that the LORD God made the earth and the heavens,\\\\n\\\\nGenesis 2:2 - And on the seventh day God finished the work that he had done, and he rested on the seventh day from all the work that he had done.\\\\n\\\\nGenesis 1:21 - So God created the great sea monsters and every living creature that moves, of every kind, with which the waters swarm, and every winged bird of every kind. And God saw that it was good.\\\\n\\\\nGenesis 1:27 - So God created humankind in his image, in the image of God he created them; male and female he created them.'}\", name='ai_rag_tool', id='cf4909cd-0bad-4bce-8984-41d277c4ed71', tool_call_id='call_sGb7YDbb41PSJdJaZVzt5ojz')]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='According to the Bible, God took six days to create the universe and then rested on the seventh day.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 536, 'total_tokens': 559, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f9f4fb6dbf', 'finish_reason': 'stop', 'logprobs': None}, id='run-4f2fddd9-f085-4312-bdcf-faf1f944b36d-0', usage_metadata={'input_tokens': 536, 'output_tokens': 23, 'total_tokens': 559, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"How many days did GOD take on creation the whole universe?\")]}\n",
        "\n",
        "async for chunk in compiled_graph.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        if node == \"action\":\n",
        "            print(f\"Tool Used: {values['messages'][0].name}\")\n",
        "        print(values[\"messages\"])\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"Could you search a map of Abraham's journey on Internet?\")]}\n",
        "\n",
        "async for chunk in compiled_graph.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        if node == \"action\":\n",
        "            print(f\"Tool Used: {values['messages'][0].name}\")\n",
        "        print(values[\"messages\"])\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"start quiz on Genesis 1:1-10?\")]}\n",
        "\n",
        "async for chunk in compiled_graph.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        if node == \"action\":\n",
        "            print(f\"Tool Used: {values['messages'][0].name}\")\n",
        "        print(values[\"messages\"])\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(Image(compiled_graph.get_graph(xray=True).draw_mermaid_png()))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llmops-course",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "041e22a9b5514e36bd4d1dac01d5d398": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d622ccc56264fac8fd7508dbdbe6e29": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "53e33aae3b97490c82aec7bbb0d6ebba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_041e22a9b5514e36bd4d1dac01d5d398",
            "placeholder": "​",
            "style": "IPY_MODEL_886d762f2a7c421382efb5502c6d42a1",
            "value": ""
          }
        },
        "716557ad09874dcb989d75f7c74424cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "72adef9b70dd48198b7322b6c5b113cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77d4c0ebaae045b58efc4f789c9a2360",
            "placeholder": "​",
            "style": "IPY_MODEL_0d622ccc56264fac8fd7508dbdbe6e29",
            "value": " 6/? [00:36&lt;00:00,  5.78s/it]"
          }
        },
        "77d4c0ebaae045b58efc4f789c9a2360": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "886d762f2a7c421382efb5502c6d42a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a61d045ffd44ac58f3f13eb10044836": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab91fd625bbd43afbf8c6398193a88d0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "ad84e0e971d3455db2efe7dd0d1f803e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab91fd625bbd43afbf8c6398193a88d0",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_716557ad09874dcb989d75f7c74424cd",
            "value": 1
          }
        },
        "efcf57067cf743d8b4ce059a61cbe02e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_53e33aae3b97490c82aec7bbb0d6ebba",
              "IPY_MODEL_ad84e0e971d3455db2efe7dd0d1f803e",
              "IPY_MODEL_72adef9b70dd48198b7322b6c5b113cf"
            ],
            "layout": "IPY_MODEL_8a61d045ffd44ac58f3f13eb10044836"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
